{"nbformat_minor": 2, "cells": [{"execution_count": 2, "cell_type": "code", "source": "%%capture\n%%configure -f\n{\"name\":\"Nacho Ria\u00f1o - Formaci\u00f3n Spark RDDs\",\n\"executorCores\":1,\n\"numExecutors\":1}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\nSparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "- `%%capture` evita que se muestre la salida por pantalla ([documentaci\u00f3n](http://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture))\n- `%%configure` establece par\u00e1metros de ejecuci\u00f3n de spark ([documentaci\u00f3n](https://github.com/jupyter-incubator/sparkmagic/blob/master/screenshots/help.png))", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "from pprint import pprint as pp #pretty printer", "outputs": [], "metadata": {"collapsed": false}}, {"source": "# RDDs simples\n\n* Similares a una lista de Python\n* Estructura de datos de solo lectura\n* Colecci\u00f3n de datos particionada y paralelizada\n* Tolerante a fallos\n* Se puede generar paralelizando una lista existente o bas\u00e1ndose en un dataset \n", "cell_type": "markdown", "metadata": {}}, {"source": "## Creaci\u00f3n", "cell_type": "markdown", "metadata": {}}, {"source": "### Creacion desde un archivo de texto", "cell_type": "markdown", "metadata": {}}, {"execution_count": 53, "cell_type": "code", "source": "rdd_quijote=sc.textFile(\"adl://barcelodatalake.azuredatalakestore.net/pruebas/Nacho/quijote-2parrafos.txt\") \npp(rdd_quijote.collect()) # Collect es una acci\u00f3n que tranforma una RDD en una lista            ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho',\n 'tiempo que viv\u00eda un hidalgo de los de lanza en astillero, adarga antigua,',\n 'roc\u00edn flaco y galgo corredor. Una olla de algo m\u00e1s vaca que carnero,',\n 'salpic\u00f3n las m\u00e1s noches, duelos y quebrantos los s\u00e1bados, lantejas los',\n 'viernes, alg\u00fan palomino de a\u00f1adidura los domingos, consum\u00edan las tres',\n 'partes de su hacienda. El resto della conclu\u00edan sayo de velarte, calzas de',\n 'velludo para las fiestas, con sus pantuflos de lo mesmo, y los d\u00edas de',\n 'entresemana se honraba con su vellor\u00ed de lo m\u00e1s fino. Ten\u00eda en su casa una',\n 'ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte,',\n 'y un mozo de campo y plaza, que as\u00ed ensillaba el roc\u00edn como tomaba la',\n 'podadera. Frisaba la edad de nuestro hidalgo con los cincuenta a\u00f1os; era de',\n 'complexi\u00f3n recia, seco de carnes, enjuto de rostro, gran madrugador y amigo',\n 'de la caza. Quieren decir que ten\u00eda el sobrenombre de Quijada, o Quesada,',\n 'que en esto hay alguna diferencia en los autores que deste caso escriben;',\n 'aunque, por conjeturas veros\u00edmiles, se deja entender que se llamaba',\n 'Quejana. Pero esto importa poco a nuestro cuento; basta que en la narraci\u00f3n',\n 'd\u00e9l no se salga un punto de la verdad.',\n '',\n 'Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba',\n 'ocioso, que eran los m\u00e1s del a\u00f1o, se daba a leer libros de caballer\u00edas, con',\n 'tanta afici\u00f3n y gusto, que olvid\u00f3 casi de todo punto el ejercicio de la',\n 'caza, y aun la administraci\u00f3n de su hacienda. Y lleg\u00f3 a tanto su curiosidad',\n 'y desatino en esto, que vendi\u00f3 muchas hanegas de tierra de sembradura para',\n 'comprar libros de caballer\u00edas en que leer, y as\u00ed, llev\u00f3 a su casa todos',\n 'cuantos pudo haber dellos; y de todos, ningunos le parec\u00edan tan bien como',\n 'los que compuso el famoso Feliciano de Silva, porque la claridad de su',\n 'prosa y aquellas entricadas razones suyas le parec\u00edan de perlas, y m\u00e1s',\n 'cuando llegaba a leer aquellos requiebros y cartas de desaf\u00edos, donde en',\n 'muchas partes hallaba escrito: La raz\u00f3n de la sinraz\u00f3n que a mi raz\u00f3n se',\n 'hace, de tal manera mi raz\u00f3n enflaquece, que con raz\u00f3n me quejo de la',\n 'vuestra fermosura. Y tambi\u00e9n cuando le\u00eda: ...los altos cielos que de',\n 'vuestra divinidad divinamente con las estrellas os fortifican, y os hacen',\n 'merecedora del merecimiento que merece la vuestra grandeza.']"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "**Ejercicio** Carga el archivo `adl://barcelodatalake.azuredatalakestore.net/pruebas/formacion_abril_2018/viajes_por_espa\u00f1a.txt`", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "rdd_viajes_espana = sc.textFile(\"adl://barcelodatalake.azuredatalakestore.net/pruebas/formacion_abril_2018/viajes_por_espa\u00f1a.txt\") ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['Si sois algo jinete (condici\u00f3n sine qua non); si cont\u00e1is adem\u00e1s con',\n 'cuatro d\u00edas y treinta duros de sobra, y ten\u00e9is, por \u00faltimo, en',\n 'Navalmoral de la Mata alg\u00fan conocido que os proporcione caballo y gu\u00eda',\n 'pod\u00e9is hacer facil\u00edsimamente un viaje de primer orden que os ofrecer\u00e1',\n 'reunidos los m\u00faltiples goces de una exploraci\u00f3n geogr\u00e1fico-pintoresca,',\n 'el grave inter\u00e9s de una excursi\u00f3n historial y art\u00edstica, y la religiosa',\n 'complacencia de aquellas romer\u00edas verdaderamente patri\u00f3ticas que, como',\n 'todo deber cumplido, ufanan y alegran el alma de los que todav\u00eda',\n 'respetan algo sobre la tierra..... Pod\u00e9is, en suma, visitar el',\n 'Monasterio de Yuste.',\n '',\n 'Para ello..... (suponemos que est\u00e1is en Madrid) empezar\u00e9is por tomar un',\n 'billete, de berlina o de interior, hasta Navalmoral de la Mata, en la',\n '\u00abDiligencia de C\u00e1ceres\u00bb, que sale diariamente de la calle del Correo',\n 'de \u00e9sta que fu\u00e9 corte, a las siete y media de la tarde.',\n '',\n 'La carretera es buena por lo general, y en ning\u00fan paraje peligrosa.',\n 'Pasar\u00e9is sucesivamente por la Dehesa de los Carabancheles, donde los',\n 'Artilleros ten\u00edan establecida su muy notable Escuela pr\u00e1ctica; por',\n 'las Ventas de Alcorc\u00f3n y por Alcorc\u00f3n mismo, que es como si',\n 'dij\u00e9ramos por el S\u00e8vres de los actuales madrile\u00f1os; por M\u00f3stoles,',\n 'donde os acordar\u00e9is de su \u00f3rgano y de su c\u00e9lebre Alcalde del a\u00f1o de',\n '1808; por Navalcarnero, uno de los principales lagares que surten de',\n 'pele\u00f3n a Madrid; por Valmojado, que nada tiene de mojado ni de valle,',\n 'pues ocupa un terreno muy alto y arcilloso; por Santa Cruz del',\n 'Retamar, abundante en fiebres intermitentes y en carbones; por',\n 'Maqueda, todav\u00eda monumental hoy, cuanto poderosa en la antig\u00fcedad',\n 'romana y en tiempos de nuestra do\u00f1a Berenguela, y, en fin, por Santa',\n 'Olalla, patria del historiador Alvar G\u00f3mez de Castro y del predicador',\n 'Crist\u00f3bal Fonseca, ambos insignes varones y literatos; con lo cual, al',\n 'amanecer (dado que viaj\u00e9is, como os lo aconsejamos, en primavera \u00f3 en',\n 'oto\u00f1o), os encontrar\u00e9is en Talavera de la Reina, confirmada (supongo)',\n 'recientemente con el nombre de Talavera de la Rep\u00fablica federal.',\n '',\n 'Dicho se est\u00e1 que en todo este trayecto no hab\u00e9is visto casi nada, a',\n 'causa de la obscuridad de la noche y de haber ido provey\u00e9ndoos de',\n 'sue\u00f1o, o bien de dormici\u00f3n o dormimiento (como se dec\u00eda anta\u00f1o,',\n 'para evitar confusiones entre la gana y el acto de dormir), y en ello',\n 'habr\u00e9is hecho perfectamente, pues no os esperan grandes hoteles, que',\n 'digamos, en toda vuestra romer\u00eda; pero al llegar a Talavera, donde se',\n 'detiene el coche una hora y se toma chocolate, despertar\u00e9is sin duda',\n 'alguna, y podr\u00e9is ver al paso muchas y muy buenas cosas.....']"}], "metadata": {"collapsed": true}}, {"source": "### Creaci\u00f3n desde una lista python: parallelize", "cell_type": "markdown", "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": "lista=[\"ab\", \"cosa\", 4.3, 'ejemplo_1', 'ejemplo_2', 45, 85]\nrdd=sc.parallelize(lista)\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:** \n\nMuestra por pantalla la lista contenida en el `rdd` previo\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "pp(rdd_viajes_espana.collect()) # Collect es una acci\u00f3n que tranforma una RDD en una lista   ", "outputs": [], "metadata": {"collapsed": true}}, {"source": "**Ejercicio:**\n\nCrea un rdd que contenga los n\u00fameros del 1 al 100  \n*pista: puedes usar funciones de python, como por ejemplo range. Asegurate que contiene los n\u00fameros 1 y 100*", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "primeros_100_numeros = range(1,100)\nrdd_primeros_100_numeros = sc.parallelize(primeros_100_numeros)\nrdd_primeros_100_numeros.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "### Almacenamiento de un rdd en el datalake: saveAsTextFile()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 49, "cell_type": "code", "source": "lista=[\"ab\", \"cosa\", 4.3, 'ejemplo_1', 'ejemplo_2', 45, 85]\nrdd=sc.parallelize(lista)\nrdd.repartition(1).saveAsTextFile(path='adl://barcelodatalake.azuredatalakestore.net/pruebas/Nacho/saveastextfile_ejemplo')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Creaci\u00f3n desde cosmos", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "from pydocumentdb import document_client\nfrom pydocumentdb import documents\n\nCOSMOSDB_MASTER_KEY = '31Q6cUXQrdSP3P19myCUZEAgTpdoru89C0mk4vo8kJZFaGAN0mPCEU5lFWwWdo4Z4FBKRMDo5WpZdw7EeP29tw=='\nconnection_policy = documents.ConnectionPolicy()\nhost = 'https://desbarcelocosmosdb.documents.azure.com:443/'\nclient = document_client.DocumentClient(host, {'masterKey': COSMOSDB_MASTER_KEY}, connection_policy)\ncollLinks = 'dbs/barcelo_des/colls/clientes_datos_basicos'\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "query = 'SELECT TOP 5 c.Datos_Comunicacion_Email.Emails[0].Email, c.Datos_Identificativos.Nombre, c.id FROM c \\\n        where c.Datos_Identificativos.Nombre != \"\"'\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "print('ejecuto la query')\nresultados_consulta_documento_cliente = client.QueryDocuments(collLinks, {'query': query, },\n                                                              {'enableCrossPartitionQuery': 'true'})\nrdd3 = sc.parallelize(resultados_consulta_documento_cliente)\npp(rdd3.collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "ejecuto la query\n[{'Email': 'enzosar@hotmail.com',\n  'Nombre': 'Enzo Adrian',\n  'id': '0940087b-7e20-484c-91c4-b61fc2dfe9f7'},\n {'Email': 'lowellkaiser@hotmail.com',\n  'Nombre': 'Lowell',\n  'id': '0dd089f9-1782-4acd-a473-21a3f1027e78'},\n {'Email': 'lorportillo@hotmail.com',\n  'Nombre': 'Lorena',\n  'id': '032a82cf-c63f-4f09-9491-82e0ddde88f0'},\n {'Email': 'stucki@sbg.at',\n  'Nombre': 'Andrea',\n  'id': '0222b07e-1a6a-4621-85bd-2d6224eb3a95'},\n {'Email': 'potterfamilymom@gmail.com',\n  'Nombre': 'Deborah Lynne',\n  'id': '04f9ab61-34db-42e0-ae20-0e46c5aca780'}]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "**Ejercicio:**\n    \nCrea y muestra por pantalla un rdd con el nombre y apellido de 10 personas con nacionalidad espa\u00f1ola  \n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "rdd = sc.parallelize(['Pablo', 'Sua\u00f1a', 'Joaquin', 'Cortes', 'Maria', 'Diez', 'Julian', 'Perez', 'Noelia', 'Martinez'])\nrdd.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['Pablo', 'Sua\u00f1a', 'Joaquin', 'Cortes', 'Maria', 'Diez', 'Julian', 'Perez', 'Noelia', 'Martinez']"}], "metadata": {"collapsed": false}}, {"source": "### Creaci\u00f3n desde lista diccionarios", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "lista=[{'clave_1': 'valor_1'}, {'clave_21': 'valor_21', 'clave_22': 'valor_22'}, {'clave_3': 'valor_3'}]\nrdd4=sc.parallelize(lista)\npp(rdd4.collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[{'clave_1': 'valor_1'},\n {'clave_21': 'valor_21', 'clave_22': 'valor_22'},\n {'clave_3': 'valor_3'}]"}], "metadata": {"collapsed": false}}, {"source": "# Transformaciones y acciones con RDDs simples", "cell_type": "markdown", "metadata": {}}, {"source": "### \u00bfQue es una transformaci\u00f3n?\n* *RDD-->RDD*: Crea un nuevo rdd a partir de uno ya existente\n* *Lazy*, es decir, todas las transformaciones ejecutadas sobre un rdd son calculadas \u00fanicamente cuando se ejecuta una acci\u00f3n\n\n### Tipos de transformaciones:\n* *Narrow*: transformaci\u00f3n que no necesita conocer los datos que est\u00e1n en el resto de nodos para  poder completarse con \u00e9xito. Ej: filter, map, etc\n* *Wide*: transformaci\u00f3n que para completarse necesita usar los datos que contienen el resto de nodos. Ej: groupBy, reduceByKey\n\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "### **Transformaciones m\u00e1s usadas:**\n\n* map(func)\n* flatMap(func)\n* filter(func)\n* sample(withReplacement,\u00a0fraction,\u00a0seed)\n* union(otherDataset)\n* intersection(otherDataset)\n* distinct([numTasks]))\n* repartitionAndSortWithinPartitions(partitioner)\n* cogroup(otherDataset, [numTasks])\n* cartesian(otherDataset)\n* coalesce(numPartitions)\n* repartition(numPartitions)\n", "cell_type": "markdown", "metadata": {}}, {"source": "## Transformaci\u00f3n Map", "cell_type": "markdown", "metadata": {}}, {"source": "### Usando lambdas (para _map_'s sencillos)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "numeros=[1,2,3,4]\nrdd=sc.parallelize(numeros)\nrdd.map(lambda x: 2*x).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[2, 4, 6, 8]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "palabras=[\"ejemplo\",\"cosa\",\"perro\"]\nrdd=sc.parallelize(palabras)\nrdd.map(lambda x: x+1).collect() #Da un error ya que el +1 no se puede aplicar a strings", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 6.0 failed 4 times, most recent failure: Lost task 8.3 in stage 6.0 (TID 181, 10.0.0.7, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 3, in <lambda>\nTypeError: Can't convert 'int' object to str implicitly\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 3, in <lambda>\nTypeError: Can't convert 'int' object to str implicitly\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 808, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 6.0 failed 4 times, most recent failure: Lost task 8.3 in stage 6.0 (TID 181, 10.0.0.7, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 3, in <lambda>\nTypeError: Can't convert 'int' object to str implicitly\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 3, in <lambda>\nTypeError: Can't convert 'int' object to str implicitly\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n"}], "metadata": {"collapsed": true}}, {"source": "### Usando funciones python para _map_'s complejos", "cell_type": "markdown", "metadata": {}}, {"execution_count": 83, "cell_type": "code", "source": "def duplicar(x):\n    factor = 2\n    resultado = factor * x\n    return resultado", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "rdd.map(duplicar).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[2, 4, 'abab', 'cosacosa', 8.6]"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "rdd.map(duplicar).map(lambda x: \"!\"+str(x)+\"!\").collect() #2 maps encadenados -cada uno de un tipo-", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['!2!', '!4!', '!abab!', '!cosacosa!', '!8.6!']"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "**Ejercicio:**\n\nA partir del rdd que contiene n\u00fameros del 1 al 100, crea un rdd cuyos elementos sean el triple de estos", "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "cell_type": "code", "source": "primeros_100_numeros = range(1,100)\nrdd_primeros_100_numeros = sc.parallelize(primeros_100_numeros)\nrdd_primeros_100_numeros_por3 = rdd_primeros_100_numeros.map(lambda numero: numero*3)\nrdd_primeros_100_numeros_por3.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99, 102, 105, 108, 111, 114, 117, 120, 123, 126, 129, 132, 135, 138, 141, 144, 147, 150, 153, 156, 159, 162, 165, 168, 171, 174, 177, 180, 183, 186, 189, 192, 195, 198, 201, 204, 207, 210, 213, 216, 219, 222, 225, 228, 231, 234, 237, 240, 243, 246, 249, 252, 255, 258, 261, 264, 267, 270, 273, 276, 279, 282, 285, 288, 291, 294, 297]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "**Ejercicio:**\n    \nA partir del rdd que contiene el texto 'Viajes por Espa\u00f1a'...\n\nA\u00f1ade el s\u00edmbolo # al principio de cada l\u00ednea y muestra el texto por pantalla", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "rdd_viajes_espana.map(lambda linea: '#'+linea).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['#Si sois algo jinete (condici\u00f3n sine qua non); si cont\u00e1is adem\u00e1s con', '#cuatro d\u00edas y treinta duros de sobra, y ten\u00e9is, por \u00faltimo, en', '#Navalmoral de la Mata alg\u00fan conocido que os proporcione caballo y gu\u00eda', '#pod\u00e9is hacer facil\u00edsimamente un viaje de primer orden que os ofrecer\u00e1', '#reunidos los m\u00faltiples goces de una exploraci\u00f3n geogr\u00e1fico-pintoresca,', '#el grave inter\u00e9s de una excursi\u00f3n historial y art\u00edstica, y la religiosa', '#complacencia de aquellas romer\u00edas verdaderamente patri\u00f3ticas que, como', '#todo deber cumplido, ufanan y alegran el alma de los que todav\u00eda', '#respetan algo sobre la tierra..... Pod\u00e9is, en suma, visitar el', '#Monasterio de Yuste.', '#', '#Para ello..... (suponemos que est\u00e1is en Madrid) empezar\u00e9is por tomar un', '#billete, de berlina o de interior, hasta Navalmoral de la Mata, en la', '#\u00abDiligencia de C\u00e1ceres\u00bb, que sale diariamente de la calle del Correo', '#de \u00e9sta que fu\u00e9 corte, a las siete y media de la tarde.', '#', '#La carretera es buena por lo general, y en ning\u00fan paraje peligrosa.', '#Pasar\u00e9is sucesivamente por la Dehesa de los Carabancheles, donde los', '#Artilleros ten\u00edan establecida su muy notable Escuela pr\u00e1ctica; por', '#las Ventas de Alcorc\u00f3n y por Alcorc\u00f3n mismo, que es como si', '#dij\u00e9ramos por el S\u00e8vres de los actuales madrile\u00f1os; por M\u00f3stoles,', '#donde os acordar\u00e9is de su \u00f3rgano y de su c\u00e9lebre Alcalde del a\u00f1o de', '#1808; por Navalcarnero, uno de los principales lagares que surten de', '#pele\u00f3n a Madrid; por Valmojado, que nada tiene de mojado ni de valle,', '#pues ocupa un terreno muy alto y arcilloso; por Santa Cruz del', '#Retamar, abundante en fiebres intermitentes y en carbones; por', '#Maqueda, todav\u00eda monumental hoy, cuanto poderosa en la antig\u00fcedad', '#romana y en tiempos de nuestra do\u00f1a Berenguela, y, en fin, por Santa', '#Olalla, patria del historiador Alvar G\u00f3mez de Castro y del predicador', '#Crist\u00f3bal Fonseca, ambos insignes varones y literatos; con lo cual, al', '#amanecer (dado que viaj\u00e9is, como os lo aconsejamos, en primavera \u00f3 en', '#oto\u00f1o), os encontrar\u00e9is en Talavera de la Reina, confirmada (supongo)', '#recientemente con el nombre de Talavera de la Rep\u00fablica federal.', '#', '#Dicho se est\u00e1 que en todo este trayecto no hab\u00e9is visto casi nada, a', '#causa de la obscuridad de la noche y de haber ido provey\u00e9ndoos de', '#sue\u00f1o, o bien de dormici\u00f3n o dormimiento (como se dec\u00eda anta\u00f1o,', '#para evitar confusiones entre la gana y el acto de dormir), y en ello', '#habr\u00e9is hecho perfectamente, pues no os esperan grandes hoteles, que', '#digamos, en toda vuestra romer\u00eda; pero al llegar a Talavera, donde se', '#detiene el coche una hora y se toma chocolate, despertar\u00e9is sin duda', '#alguna, y podr\u00e9is ver al paso muchas y muy buenas cosas.....']"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**\n    \nA partir del rdd que contiene el texto 'Viajes por Espa\u00f1a'...  \nSepara cada l\u00ednea en palabras (entendemos por palabra un texto delimitado por espacios) y muestralo por pantalla. Deber\u00edas obtener una lista de listas  \nPista: usar el m\u00e9todo split() de python", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "rdd_viajes_espana.map(lambda linea: linea.split()).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[['Si', 'sois', 'algo', 'jinete', '(condici\u00f3n', 'sine', 'qua', 'non);', 'si', 'cont\u00e1is', 'adem\u00e1s', 'con'], ['cuatro', 'd\u00edas', 'y', 'treinta', 'duros', 'de', 'sobra,', 'y', 'ten\u00e9is,', 'por', '\u00faltimo,', 'en'], ['Navalmoral', 'de', 'la', 'Mata', 'alg\u00fan', 'conocido', 'que', 'os', 'proporcione', 'caballo', 'y', 'gu\u00eda'], ['pod\u00e9is', 'hacer', 'facil\u00edsimamente', 'un', 'viaje', 'de', 'primer', 'orden', 'que', 'os', 'ofrecer\u00e1'], ['reunidos', 'los', 'm\u00faltiples', 'goces', 'de', 'una', 'exploraci\u00f3n', 'geogr\u00e1fico-pintoresca,'], ['el', 'grave', 'inter\u00e9s', 'de', 'una', 'excursi\u00f3n', 'historial', 'y', 'art\u00edstica,', 'y', 'la', 'religiosa'], ['complacencia', 'de', 'aquellas', 'romer\u00edas', 'verdaderamente', 'patri\u00f3ticas', 'que,', 'como'], ['todo', 'deber', 'cumplido,', 'ufanan', 'y', 'alegran', 'el', 'alma', 'de', 'los', 'que', 'todav\u00eda'], ['respetan', 'algo', 'sobre', 'la', 'tierra.....', 'Pod\u00e9is,', 'en', 'suma,', 'visitar', 'el'], ['Monasterio', 'de', 'Yuste.'], [], ['Para', 'ello.....', '(suponemos', 'que', 'est\u00e1is', 'en', 'Madrid)', 'empezar\u00e9is', 'por', 'tomar', 'un'], ['billete,', 'de', 'berlina', 'o', 'de', 'interior,', 'hasta', 'Navalmoral', 'de', 'la', 'Mata,', 'en', 'la'], ['\u00abDiligencia', 'de', 'C\u00e1ceres\u00bb,', 'que', 'sale', 'diariamente', 'de', 'la', 'calle', 'del', 'Correo'], ['de', '\u00e9sta', 'que', 'fu\u00e9', 'corte,', 'a', 'las', 'siete', 'y', 'media', 'de', 'la', 'tarde.'], [], ['La', 'carretera', 'es', 'buena', 'por', 'lo', 'general,', 'y', 'en', 'ning\u00fan', 'paraje', 'peligrosa.'], ['Pasar\u00e9is', 'sucesivamente', 'por', 'la', 'Dehesa', 'de', 'los', 'Carabancheles,', 'donde', 'los'], ['Artilleros', 'ten\u00edan', 'establecida', 'su', 'muy', 'notable', 'Escuela', 'pr\u00e1ctica;', 'por'], ['las', 'Ventas', 'de', 'Alcorc\u00f3n', 'y', 'por', 'Alcorc\u00f3n', 'mismo,', 'que', 'es', 'como', 'si'], ['dij\u00e9ramos', 'por', 'el', 'S\u00e8vres', 'de', 'los', 'actuales', 'madrile\u00f1os;', 'por', 'M\u00f3stoles,'], ['donde', 'os', 'acordar\u00e9is', 'de', 'su', '\u00f3rgano', 'y', 'de', 'su', 'c\u00e9lebre', 'Alcalde', 'del', 'a\u00f1o', 'de'], ['1808;', 'por', 'Navalcarnero,', 'uno', 'de', 'los', 'principales', 'lagares', 'que', 'surten', 'de'], ['pele\u00f3n', 'a', 'Madrid;', 'por', 'Valmojado,', 'que', 'nada', 'tiene', 'de', 'mojado', 'ni', 'de', 'valle,'], ['pues', 'ocupa', 'un', 'terreno', 'muy', 'alto', 'y', 'arcilloso;', 'por', 'Santa', 'Cruz', 'del'], ['Retamar,', 'abundante', 'en', 'fiebres', 'intermitentes', 'y', 'en', 'carbones;', 'por'], ['Maqueda,', 'todav\u00eda', 'monumental', 'hoy,', 'cuanto', 'poderosa', 'en', 'la', 'antig\u00fcedad'], ['romana', 'y', 'en', 'tiempos', 'de', 'nuestra', 'do\u00f1a', 'Berenguela,', 'y,', 'en', 'fin,', 'por', 'Santa'], ['Olalla,', 'patria', 'del', 'historiador', 'Alvar', 'G\u00f3mez', 'de', 'Castro', 'y', 'del', 'predicador'], ['Crist\u00f3bal', 'Fonseca,', 'ambos', 'insignes', 'varones', 'y', 'literatos;', 'con', 'lo', 'cual,', 'al'], ['amanecer', '(dado', 'que', 'viaj\u00e9is,', 'como', 'os', 'lo', 'aconsejamos,', 'en', 'primavera', '\u00f3', 'en'], ['oto\u00f1o),', 'os', 'encontrar\u00e9is', 'en', 'Talavera', 'de', 'la', 'Reina,', 'confirmada', '(supongo)'], ['recientemente', 'con', 'el', 'nombre', 'de', 'Talavera', 'de', 'la', 'Rep\u00fablica', 'federal.'], [], ['Dicho', 'se', 'est\u00e1', 'que', 'en', 'todo', 'este', 'trayecto', 'no', 'hab\u00e9is', 'visto', 'casi', 'nada,', 'a'], ['causa', 'de', 'la', 'obscuridad', 'de', 'la', 'noche', 'y', 'de', 'haber', 'ido', 'provey\u00e9ndoos', 'de'], ['sue\u00f1o,', 'o', 'bien', 'de', 'dormici\u00f3n', 'o', 'dormimiento', '(como', 'se', 'dec\u00eda', 'anta\u00f1o,'], ['para', 'evitar', 'confusiones', 'entre', 'la', 'gana', 'y', 'el', 'acto', 'de', 'dormir),', 'y', 'en', 'ello'], ['habr\u00e9is', 'hecho', 'perfectamente,', 'pues', 'no', 'os', 'esperan', 'grandes', 'hoteles,', 'que'], ['digamos,', 'en', 'toda', 'vuestra', 'romer\u00eda;', 'pero', 'al', 'llegar', 'a', 'Talavera,', 'donde', 'se'], ['detiene', 'el', 'coche', 'una', 'hora', 'y', 'se', 'toma', 'chocolate,', 'despertar\u00e9is', 'sin', 'duda'], ['alguna,', 'y', 'podr\u00e9is', 'ver', 'al', 'paso', 'muchas', 'y', 'muy', 'buenas', 'cosas.....']]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n flatMap", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "def factores(x):\n    lista_factores=[]   \n    for i in range(1, x + 1):\n        if x % i == 0:\n            lista_factores.append(i)\n            \n    \n    return lista_factores         \n\nrdd=sc.parallelize([1,15,21,77])\nprint('map normal: \\n', rdd.map(factores).collect(),'\\n')\n\nprint('flatMap: \\n',rdd.flatMap(factores).collect(), '\\n')", "outputs": [{"output_type": "stream", "name": "stdout", "text": "map normal: \n [[1], [1, 3, 5, 15], [1, 3, 7, 21], [1, 7, 11, 77]] \n\nflatMap: \n [1, 1, 3, 5, 15, 1, 3, 7, 21, 1, 7, 11, 77]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**\n    \nA partir del rdd que contiene el texto 'Viajes por Espa\u00f1a'...\n\nSepara cada l\u00ednea en palabras (entendemos por palabra lo delimitados por espacios) y obten esta vez una lista de strings  \nMuestra el resultado por pantalla\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 113, "cell_type": "code", "source": "rdd_palabras_viajes_espana = rdd_viajes_espana.flatMap(lambda linea: linea.split())\nrdd_palabras_viajes_espana.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['Si', 'sois', 'algo', 'jinete', '(condici\u00f3n', 'sine', 'qua', 'non);', 'si', 'cont\u00e1is', 'adem\u00e1s', 'con', 'cuatro', 'd\u00edas', 'y', 'treinta', 'duros', 'de', 'sobra,', 'y', 'ten\u00e9is,', 'por', '\u00faltimo,', 'en', 'Navalmoral', 'de', 'la', 'Mata', 'alg\u00fan', 'conocido', 'que', 'os', 'proporcione', 'caballo', 'y', 'gu\u00eda', 'pod\u00e9is', 'hacer', 'facil\u00edsimamente', 'un', 'viaje', 'de', 'primer', 'orden', 'que', 'os', 'ofrecer\u00e1', 'reunidos', 'los', 'm\u00faltiples', 'goces', 'de', 'una', 'exploraci\u00f3n', 'geogr\u00e1fico-pintoresca,', 'el', 'grave', 'inter\u00e9s', 'de', 'una', 'excursi\u00f3n', 'historial', 'y', 'art\u00edstica,', 'y', 'la', 'religiosa', 'complacencia', 'de', 'aquellas', 'romer\u00edas', 'verdaderamente', 'patri\u00f3ticas', 'que,', 'como', 'todo', 'deber', 'cumplido,', 'ufanan', 'y', 'alegran', 'el', 'alma', 'de', 'los', 'que', 'todav\u00eda', 'respetan', 'algo', 'sobre', 'la', 'tierra.....', 'Pod\u00e9is,', 'en', 'suma,', 'visitar', 'el', 'Monasterio', 'de', 'Yuste.', 'Para', 'ello.....', '(suponemos', 'que', 'est\u00e1is', 'en', 'Madrid)', 'empezar\u00e9is', 'por', 'tomar', 'un', 'billete,', 'de', 'berlina', 'o', 'de', 'interior,', 'hasta', 'Navalmoral', 'de', 'la', 'Mata,', 'en', 'la', '\u00abDiligencia', 'de', 'C\u00e1ceres\u00bb,', 'que', 'sale', 'diariamente', 'de', 'la', 'calle', 'del', 'Correo', 'de', '\u00e9sta', 'que', 'fu\u00e9', 'corte,', 'a', 'las', 'siete', 'y', 'media', 'de', 'la', 'tarde.', 'La', 'carretera', 'es', 'buena', 'por', 'lo', 'general,', 'y', 'en', 'ning\u00fan', 'paraje', 'peligrosa.', 'Pasar\u00e9is', 'sucesivamente', 'por', 'la', 'Dehesa', 'de', 'los', 'Carabancheles,', 'donde', 'los', 'Artilleros', 'ten\u00edan', 'establecida', 'su', 'muy', 'notable', 'Escuela', 'pr\u00e1ctica;', 'por', 'las', 'Ventas', 'de', 'Alcorc\u00f3n', 'y', 'por', 'Alcorc\u00f3n', 'mismo,', 'que', 'es', 'como', 'si', 'dij\u00e9ramos', 'por', 'el', 'S\u00e8vres', 'de', 'los', 'actuales', 'madrile\u00f1os;', 'por', 'M\u00f3stoles,', 'donde', 'os', 'acordar\u00e9is', 'de', 'su', '\u00f3rgano', 'y', 'de', 'su', 'c\u00e9lebre', 'Alcalde', 'del', 'a\u00f1o', 'de', '1808;', 'por', 'Navalcarnero,', 'uno', 'de', 'los', 'principales', 'lagares', 'que', 'surten', 'de', 'pele\u00f3n', 'a', 'Madrid;', 'por', 'Valmojado,', 'que', 'nada', 'tiene', 'de', 'mojado', 'ni', 'de', 'valle,', 'pues', 'ocupa', 'un', 'terreno', 'muy', 'alto', 'y', 'arcilloso;', 'por', 'Santa', 'Cruz', 'del', 'Retamar,', 'abundante', 'en', 'fiebres', 'intermitentes', 'y', 'en', 'carbones;', 'por', 'Maqueda,', 'todav\u00eda', 'monumental', 'hoy,', 'cuanto', 'poderosa', 'en', 'la', 'antig\u00fcedad', 'romana', 'y', 'en', 'tiempos', 'de', 'nuestra', 'do\u00f1a', 'Berenguela,', 'y,', 'en', 'fin,', 'por', 'Santa', 'Olalla,', 'patria', 'del', 'historiador', 'Alvar', 'G\u00f3mez', 'de', 'Castro', 'y', 'del', 'predicador', 'Crist\u00f3bal', 'Fonseca,', 'ambos', 'insignes', 'varones', 'y', 'literatos;', 'con', 'lo', 'cual,', 'al', 'amanecer', '(dado', 'que', 'viaj\u00e9is,', 'como', 'os', 'lo', 'aconsejamos,', 'en', 'primavera', '\u00f3', 'en', 'oto\u00f1o),', 'os', 'encontrar\u00e9is', 'en', 'Talavera', 'de', 'la', 'Reina,', 'confirmada', '(supongo)', 'recientemente', 'con', 'el', 'nombre', 'de', 'Talavera', 'de', 'la', 'Rep\u00fablica', 'federal.', 'Dicho', 'se', 'est\u00e1', 'que', 'en', 'todo', 'este', 'trayecto', 'no', 'hab\u00e9is', 'visto', 'casi', 'nada,', 'a', 'causa', 'de', 'la', 'obscuridad', 'de', 'la', 'noche', 'y', 'de', 'haber', 'ido', 'provey\u00e9ndoos', 'de', 'sue\u00f1o,', 'o', 'bien', 'de', 'dormici\u00f3n', 'o', 'dormimiento', '(como', 'se', 'dec\u00eda', 'anta\u00f1o,', 'para', 'evitar', 'confusiones', 'entre', 'la', 'gana', 'y', 'el', 'acto', 'de', 'dormir),', 'y', 'en', 'ello', 'habr\u00e9is', 'hecho', 'perfectamente,', 'pues', 'no', 'os', 'esperan', 'grandes', 'hoteles,', 'que', 'digamos,', 'en', 'toda', 'vuestra', 'romer\u00eda;', 'pero', 'al', 'llegar', 'a', 'Talavera,', 'donde', 'se', 'detiene', 'el', 'coche', 'una', 'hora', 'y', 'se', 'toma', 'chocolate,', 'despertar\u00e9is', 'sin', 'duda', 'alguna,', 'y', 'podr\u00e9is', 'ver', 'al', 'paso', 'muchas', 'y', 'muy', 'buenas', 'cosas.....']"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n mapPartitions", "cell_type": "markdown", "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8], 3)\nprint(\"particiones\",rdd.glom().collect())\ndef s(iterator): yield sum(iterator)\ndef m(iterator): yield max(iterator)\n\nprint(\"suma\",rdd.mapPartitions(s).collect())\nprint(\"suma\",rdd.mapPartitions(m).collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "particiones [[1, 2], [3, 4], [5, 6, 7, 8]]\nsuma [3, 7, 26]\nsuma [2, 4, 8]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "## Transformaci\u00f3n filter", "cell_type": "markdown", "metadata": {}}, {"execution_count": 88, "cell_type": "code", "source": "rdd = sc.parallelize(['a', 'b', 1, 13, 'ab', 12])\n\nprint('rdd original: ', rdd.collect())\nprint('rdd filtrado: ', rdd.filter(lambda x:  type(x) is not str ).collect())\n\nprint('rdd filtrado y aplicado un map: ', \n      rdd.filter(lambda x:  type(x) is not str )\\\n      .map(duplicar).collect()) # Todas las transformaciones se pueden encadenar", "outputs": [{"output_type": "stream", "name": "stdout", "text": "rdd original:  ['a', 'b', 1, 13, 'ab', 12]\nrdd filtrado:  [1, 13, 12]\nrdd filtrado y aplicado un map:  [2, 26, 24]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**\n    \nA partir del rdd que contiene los numeros del 1 al 100 encuentra los divisibles por 7", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "rdd_primeros_100_numeros.filter(lambda numero: numero%7 == 0).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**\n    \nA partir del rdd que contiene el texto 'Viajes por Espa\u00f1a'... encuentra:\n* Las l\u00edneas que contienen hotel u hoteles    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "rdd_viajes_espana.filter(lambda linea: 'hotel' in linea).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['habr\u00e9is hecho perfectamente, pues no os esperan grandes hoteles, que']"}], "metadata": {"collapsed": false}}, {"source": "* Las palabras de menos de 6 letras  \nPista: dividir la l\u00ednea en palabras previamente y usar la funci\u00f3n de Python 'len'\n    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "rdd_viajes_espana.flatMap(lambda linea: linea.split()).filter(lambda palabra: len(palabra)<6).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['Si', 'sois', 'algo', 'sine', 'qua', 'non);', 'si', 'con', 'd\u00edas', 'y', 'duros', 'de', 'y', 'por', 'en', 'de', 'la', 'Mata', 'alg\u00fan', 'que', 'os', 'y', 'gu\u00eda', 'hacer', 'un', 'viaje', 'de', 'orden', 'que', 'os', 'los', 'goces', 'de', 'una', 'el', 'grave', 'de', 'una', 'y', 'y', 'la', 'de', 'que,', 'como', 'todo', 'deber', 'y', 'el', 'alma', 'de', 'los', 'que', 'algo', 'sobre', 'la', 'en', 'suma,', 'el', 'de', 'Para', 'que', 'en', 'por', 'tomar', 'un', 'de', 'o', 'de', 'hasta', 'de', 'la', 'Mata,', 'en', 'la', 'de', 'que', 'sale', 'de', 'la', 'calle', 'del', 'de', '\u00e9sta', 'que', 'fu\u00e9', 'a', 'las', 'siete', 'y', 'media', 'de', 'la', 'La', 'es', 'buena', 'por', 'lo', 'y', 'en', 'por', 'la', 'de', 'los', 'donde', 'los', 'su', 'muy', 'por', 'las', 'de', 'y', 'por', 'que', 'es', 'como', 'si', 'por', 'el', 'de', 'los', 'por', 'donde', 'os', 'de', 'su', 'y', 'de', 'su', 'del', 'a\u00f1o', 'de', '1808;', 'por', 'uno', 'de', 'los', 'que', 'de', 'a', 'por', 'que', 'nada', 'tiene', 'de', 'ni', 'de', 'pues', 'ocupa', 'un', 'muy', 'alto', 'y', 'por', 'Santa', 'Cruz', 'del', 'en', 'y', 'en', 'por', 'hoy,', 'en', 'la', 'y', 'en', 'de', 'do\u00f1a', 'y,', 'en', 'fin,', 'por', 'Santa', 'del', 'Alvar', 'G\u00f3mez', 'de', 'y', 'del', 'ambos', 'y', 'con', 'lo', 'cual,', 'al', '(dado', 'que', 'como', 'os', 'lo', 'en', '\u00f3', 'en', 'os', 'en', 'de', 'la', 'con', 'el', 'de', 'de', 'la', 'Dicho', 'se', 'est\u00e1', 'que', 'en', 'todo', 'este', 'no', 'visto', 'casi', 'nada,', 'a', 'causa', 'de', 'la', 'de', 'la', 'noche', 'y', 'de', 'haber', 'ido', 'de', 'o', 'bien', 'de', 'o', '(como', 'se', 'dec\u00eda', 'para', 'entre', 'la', 'gana', 'y', 'el', 'acto', 'de', 'y', 'en', 'ello', 'hecho', 'pues', 'no', 'os', 'que', 'en', 'toda', 'pero', 'al', 'a', 'donde', 'se', 'el', 'coche', 'una', 'hora', 'y', 'se', 'toma', 'sin', 'duda', 'y', 'ver', 'al', 'paso', 'y', 'muy']"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**\n    \nA partir del rdd que contiene el texto 'Viajes por Espa\u00f1a'...\n\nSepara cada l\u00ednea en palabras (entendemos por palabra lo delimitados por espacios).\nDespues, convierte cada palabra a minusculas y elimina los caracteres no alfanum\u00e9ricos.\nFinalmente, muestra el resultado por pantalla  \nPista: Para eliminar los car\u00e1cteres no alfanumericos usar la expresi\u00f3n regular '[^\\w]' y la funci\u00f3n *sub* de la librer\u00eda re  \n       Para poner en min\u00fascula se usa el m\u00e9todo lower()\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 117, "cell_type": "code", "source": "import re\nrdd_palabras_alfanum_viajes_espana = rdd_viajes_espana.flatMap(lambda linea: linea.split()).map(lambda palabra: re.sub('[^\\w]', '', palabra.lower()))\nrdd_palabras_alfanum_viajes_espana.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['si', 'sois', 'algo', 'jinete', 'condici\u00f3n', 'sine', 'qua', 'non', 'si', 'cont\u00e1is', 'adem\u00e1s', 'con', 'cuatro', 'd\u00edas', 'y', 'treinta', 'duros', 'de', 'sobra', 'y', 'ten\u00e9is', 'por', '\u00faltimo', 'en', 'navalmoral', 'de', 'la', 'mata', 'alg\u00fan', 'conocido', 'que', 'os', 'proporcione', 'caballo', 'y', 'gu\u00eda', 'pod\u00e9is', 'hacer', 'facil\u00edsimamente', 'un', 'viaje', 'de', 'primer', 'orden', 'que', 'os', 'ofrecer\u00e1', 'reunidos', 'los', 'm\u00faltiples', 'goces', 'de', 'una', 'exploraci\u00f3n', 'geogr\u00e1ficopintoresca', 'el', 'grave', 'inter\u00e9s', 'de', 'una', 'excursi\u00f3n', 'historial', 'y', 'art\u00edstica', 'y', 'la', 'religiosa', 'complacencia', 'de', 'aquellas', 'romer\u00edas', 'verdaderamente', 'patri\u00f3ticas', 'que', 'como', 'todo', 'deber', 'cumplido', 'ufanan', 'y', 'alegran', 'el', 'alma', 'de', 'los', 'que', 'todav\u00eda', 'respetan', 'algo', 'sobre', 'la', 'tierra', 'pod\u00e9is', 'en', 'suma', 'visitar', 'el', 'monasterio', 'de', 'yuste', 'para', 'ello', 'suponemos', 'que', 'est\u00e1is', 'en', 'madrid', 'empezar\u00e9is', 'por', 'tomar', 'un', 'billete', 'de', 'berlina', 'o', 'de', 'interior', 'hasta', 'navalmoral', 'de', 'la', 'mata', 'en', 'la', 'diligencia', 'de', 'c\u00e1ceres', 'que', 'sale', 'diariamente', 'de', 'la', 'calle', 'del', 'correo', 'de', '\u00e9sta', 'que', 'fu\u00e9', 'corte', 'a', 'las', 'siete', 'y', 'media', 'de', 'la', 'tarde', 'la', 'carretera', 'es', 'buena', 'por', 'lo', 'general', 'y', 'en', 'ning\u00fan', 'paraje', 'peligrosa', 'pasar\u00e9is', 'sucesivamente', 'por', 'la', 'dehesa', 'de', 'los', 'carabancheles', 'donde', 'los', 'artilleros', 'ten\u00edan', 'establecida', 'su', 'muy', 'notable', 'escuela', 'pr\u00e1ctica', 'por', 'las', 'ventas', 'de', 'alcorc\u00f3n', 'y', 'por', 'alcorc\u00f3n', 'mismo', 'que', 'es', 'como', 'si', 'dij\u00e9ramos', 'por', 'el', 's\u00e8vres', 'de', 'los', 'actuales', 'madrile\u00f1os', 'por', 'm\u00f3stoles', 'donde', 'os', 'acordar\u00e9is', 'de', 'su', '\u00f3rgano', 'y', 'de', 'su', 'c\u00e9lebre', 'alcalde', 'del', 'a\u00f1o', 'de', '1808', 'por', 'navalcarnero', 'uno', 'de', 'los', 'principales', 'lagares', 'que', 'surten', 'de', 'pele\u00f3n', 'a', 'madrid', 'por', 'valmojado', 'que', 'nada', 'tiene', 'de', 'mojado', 'ni', 'de', 'valle', 'pues', 'ocupa', 'un', 'terreno', 'muy', 'alto', 'y', 'arcilloso', 'por', 'santa', 'cruz', 'del', 'retamar', 'abundante', 'en', 'fiebres', 'intermitentes', 'y', 'en', 'carbones', 'por', 'maqueda', 'todav\u00eda', 'monumental', 'hoy', 'cuanto', 'poderosa', 'en', 'la', 'antig\u00fcedad', 'romana', 'y', 'en', 'tiempos', 'de', 'nuestra', 'do\u00f1a', 'berenguela', 'y', 'en', 'fin', 'por', 'santa', 'olalla', 'patria', 'del', 'historiador', 'alvar', 'g\u00f3mez', 'de', 'castro', 'y', 'del', 'predicador', 'crist\u00f3bal', 'fonseca', 'ambos', 'insignes', 'varones', 'y', 'literatos', 'con', 'lo', 'cual', 'al', 'amanecer', 'dado', 'que', 'viaj\u00e9is', 'como', 'os', 'lo', 'aconsejamos', 'en', 'primavera', '\u00f3', 'en', 'oto\u00f1o', 'os', 'encontrar\u00e9is', 'en', 'talavera', 'de', 'la', 'reina', 'confirmada', 'supongo', 'recientemente', 'con', 'el', 'nombre', 'de', 'talavera', 'de', 'la', 'rep\u00fablica', 'federal', 'dicho', 'se', 'est\u00e1', 'que', 'en', 'todo', 'este', 'trayecto', 'no', 'hab\u00e9is', 'visto', 'casi', 'nada', 'a', 'causa', 'de', 'la', 'obscuridad', 'de', 'la', 'noche', 'y', 'de', 'haber', 'ido', 'provey\u00e9ndoos', 'de', 'sue\u00f1o', 'o', 'bien', 'de', 'dormici\u00f3n', 'o', 'dormimiento', 'como', 'se', 'dec\u00eda', 'anta\u00f1o', 'para', 'evitar', 'confusiones', 'entre', 'la', 'gana', 'y', 'el', 'acto', 'de', 'dormir', 'y', 'en', 'ello', 'habr\u00e9is', 'hecho', 'perfectamente', 'pues', 'no', 'os', 'esperan', 'grandes', 'hoteles', 'que', 'digamos', 'en', 'toda', 'vuestra', 'romer\u00eda', 'pero', 'al', 'llegar', 'a', 'talavera', 'donde', 'se', 'detiene', 'el', 'coche', 'una', 'hora', 'y', 'se', 'toma', 'chocolate', 'despertar\u00e9is', 'sin', 'duda', 'alguna', 'y', 'podr\u00e9is', 'ver', 'al', 'paso', 'muchas', 'y', 'muy', 'buenas', 'cosas']"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n sample", "cell_type": "markdown", "metadata": {}}, {"execution_count": 44, "cell_type": "code", "source": "rdd = sc.parallelize(['a','b','c', 'd','e','f', 1,2,3,4])\n\nprint(rdd.sample(withReplacement=False, fraction=0.2).collect())\nprint(rdd.sample(withReplacement=True, fraction=0.5).collect())\nprint(rdd.sample(withReplacement=True, fraction=0.5).collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1, 2]\n['a', 'c', 'c']\n['a', 'a', 'b', 'f', 1, 2, 3, 4]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n union", "cell_type": "markdown", "metadata": {}}, {"execution_count": 122, "cell_type": "code", "source": "rdd_1 = sc.parallelize([1,2,3, 4])\nrdd_2 = sc.parallelize([1, 11,22,33, 4])\n\nrdd = rdd_1.union(rdd_2)\nrdd.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1, 2, 3, 4, 11, 22, 33, 4]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n intersection", "cell_type": "markdown", "metadata": {}}, {"execution_count": 121, "cell_type": "code", "source": "rdd_1 = sc.parallelize([1, 2, 3, 4])\nrdd_2 = sc.parallelize([1, 11,22,33, 4])\n\nrdd = rdd_1.intersection(rdd_2)\nrdd.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[4]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**\n    \nA partir del rdd generado con los valores multiplicados por tres y el rdd original construido a partir de la lista de valores del 1 al 100...\n\n* Tenerar un rdd que sea la uni\u00f3n de ambos    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 39, "cell_type": "code", "source": "rdd_primeros_100_numeros.union(rdd_primeros_100_numeros_por3).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99, 102, 105, 108, 111, 114, 117, 120, 123, 126, 129, 132, 135, 138, 141, 144, 147, 150, 153, 156, 159, 162, 165, 168, 171, 174, 177, 180, 183, 186, 189, 192, 195, 198, 201, 204, 207, 210, 213, 216, 219, 222, 225, 228, 231, 234, 237, 240, 243, 246, 249, 252, 255, 258, 261, 264, 267, 270, 273, 276, 279, 282, 285, 288, 291, 294, 297]"}], "metadata": {"collapsed": false}}, {"source": "* Generar otro rdd que sea la intersecci\u00f3n entre ambos\n\n\n    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 41, "cell_type": "code", "source": "sorted(rdd_primeros_100_numeros.intersection(rdd_primeros_100_numeros_por3).collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99]"}], "metadata": {"collapsed": false}}, {"source": "* Tomar una muestra de registros al azar (semilla = 1) con fraction=0.2 sin reemplazamiento del rdd con los valores del 1 al 100 y hacer la union y la intersecci\u00f3n con el rdd de valores * 3\n\n\n    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 50, "cell_type": "code", "source": "rdd_random = rdd_primeros_100_numeros.sample(withReplacement=False, fraction=0.2, seed=1)\nrdd_random.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[10, 12, 14, 31, 42, 64, 74, 79, 85, 91, 95]"}], "metadata": {"collapsed": false}}, {"execution_count": 51, "cell_type": "code", "source": "rdd_random.union(rdd_primeros_100_numeros_por3).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[10, 12, 14, 31, 42, 64, 74, 79, 85, 91, 95, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99, 102, 105, 108, 111, 114, 117, 120, 123, 126, 129, 132, 135, 138, 141, 144, 147, 150, 153, 156, 159, 162, 165, 168, 171, 174, 177, 180, 183, 186, 189, 192, 195, 198, 201, 204, 207, 210, 213, 216, 219, 222, 225, 228, 231, 234, 237, 240, 243, 246, 249, 252, 255, 258, 261, 264, 267, 270, 273, 276, 279, 282, 285, 288, 291, 294, 297]"}], "metadata": {"collapsed": false}}, {"execution_count": 52, "cell_type": "code", "source": "rdd_random.intersection(rdd_primeros_100_numeros_por3).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[12, 42]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**\nObt\u00e9n una lista de las palabras que aparecen en ambos textos (\"El quijote\" y \"Viajes por Espa\u00f1a\").  \n*Pista: recuerda que es necesario normalizar las palabras antes...*", "cell_type": "markdown", "metadata": {}}, {"execution_count": 63, "cell_type": "code", "source": "rdd_viajes_espana_norm = rdd_viajes_espana.flatMap(lambda linea: linea.split()).map(lambda palabra: palabra.strip().lower())\nrdd_quijote_norm = rdd_quijote.flatMap(lambda linea: linea.split()).map(lambda palabra: palabra.strip().lower())\nsorted(rdd_viajes_espana_norm.intersection(rdd_quijote_norm).collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['a', 'algo', 'alg\u00fan', 'aquellas', 'bien', 'casi', 'como', 'con', 'de', 'del', 'donde', 'd\u00edas', 'el', 'en', 'este', 'haber', 'la', 'las', 'lo', 'los', 'muchas', 'no', 'nombre', 'o', 'os', 'para', 'pero', 'por', 'que', 'se', 'su', 'todo', 'un', 'una', 'vuestra', 'y']"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n distinct", "cell_type": "markdown", "metadata": {}}, {"execution_count": 65, "cell_type": "code", "source": "def factores(x):\n    lista_factores=[]   \n    for i in range(1, x + 1):\n        if x % i == 0:\n            lista_factores.append(i)\n                        \n    \n    return lista_factores         \n\nnumeros=[1,15,21,77]\nrdd=sc.parallelize(numeros)\nprint('antes del distinct:\\n', rdd.flatMap(factores).collect())\nprint('\\n despues del distinct: ', rdd.flatMap(factores).distinct().collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "antes del distinct:\n [1, 1, 3, 5, 15, 1, 3, 7, 21, 1, 7, 11, 77]\n\n despues del distinct:  [11, 1, 3, 21, 7, 15, 77, 5]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "**Ejercicio:**\n    \nEn el ejemplo previo sobre `distinct`, se llama a flatMap dos veces, lo cual es ineficiente -y por lo tanto una mala pr\u00e1ctica-  \nCorrije el ejemplo.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 66, "cell_type": "code", "source": "rdd_flatmapeado = rdd.flatMap(factores)\n\nprint('antes del distinct:\\n', rdd_flatmapeado.collect())\nprint('\\n despues del distinct: ', rdd_flatmapeado.distinct().collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "antes del distinct:\n [1, 1, 3, 5, 15, 1, 3, 7, 21, 1, 7, 11, 77]\n\n despues del distinct:  [11, 1, 3, 21, 7, 15, 77, 5]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n cartesian", "cell_type": "markdown", "metadata": {}}, {"execution_count": 72, "cell_type": "code", "source": "rdd1 = sc.parallelize([1, 2])\nrdd2 = sc.parallelize([3, 4])\nsorted(rdd1.cartesian(rdd2).collect())\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(1, 3), (1, 4), (2, 3), (2, 4)]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio**:  \nGenerar un rdd con todos los enfrentamientos posibles entre estos tres \nequipos:\u201dIbiza FC\u201d, \u201dMallorca FC\u201d y \u201dMenorca FC\u201d\n\nHazlo de dos maneras:\n* No nos importa que un equipo se enfrente a s\u00ed mismo\n", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 76, "cell_type": "code", "source": "rdd_1 = sc.parallelize(['Ibiza', 'Mallorca', 'Menorca'])\nrdd_2 = sc.parallelize(['Ibiza', 'Mallorca', 'Menorca'])\n\nrdd_enfrentamientos = rdd_1.cartesian(rdd_2)\nrdd_enfrentamientos.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('Ibiza', 'Ibiza'), ('Ibiza', 'Mallorca'), ('Ibiza', 'Menorca'), ('Mallorca', 'Ibiza'), ('Menorca', 'Ibiza'), ('Mallorca', 'Mallorca'), ('Mallorca', 'Menorca'), ('Menorca', 'Mallorca'), ('Menorca', 'Menorca')]"}], "metadata": {"collapsed": false}}, {"source": "* No queremos que un equipo pueda enfrentarse a s\u00ed mismo\n", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 77, "cell_type": "code", "source": "rdd_enfrentamientos.filter(lambda elemento: elemento[0] != elemento[1]).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('Ibiza', 'Mallorca'), ('Ibiza', 'Menorca'), ('Mallorca', 'Ibiza'), ('Menorca', 'Ibiza'), ('Mallorca', 'Menorca'), ('Menorca', 'Mallorca')]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n coalesce y repartition", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "rdd1 = sc.parallelize([1, 2, 3, 4, 14, 15, 16, 17, 18, 19, 20, 5, 6, 7, 8, 9, 10, 11, 12, 13], 4)\n\nprint('rdd original:\\n', rdd1.glom().collect())\nprint('\\nrdd coalesce:\\n', rdd1.coalesce(3).glom().collect())\nprint('\\nrdd repartition:\\n', rdd1.repartition(3).glom().collect())\nprint('\\nrdd repartition:\\n', rdd1.repartition(6).glom().collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "rdd original:\n [[1, 2, 3, 4, 14], [15, 16, 17, 18, 19], [20, 5, 6, 7, 8], [9, 10, 11, 12, 13]]\n\nrdd coalesce:\n [[1, 2, 3, 4, 14], [15, 16, 17, 18, 19], [20, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n\nrdd repartition:\n [[9, 10, 11, 12, 13], [1, 2, 3, 4, 14, 15, 16, 17, 18, 19], [20, 5, 6, 7, 8]]\n\nrdd repartition:\n [[], [1, 2, 3, 4, 14], [], [9, 10, 11, 12, 13], [15, 16, 17, 18, 19], [20, 5, 6, 7, 8]]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "## Transformaci\u00f3n Sample", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "import random\n\ndef crear_1000_numeros_aleatorios():\n    numeros=[random.randint(0,42) for x in range(0,1000)]\n    return numeros\n\nrdd=sc.parallelize(range(0,100)).flatMap(lambda _:crear_1000_numeros_aleatorios())\n    ", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 45, "cell_type": "code", "source": "sampled_rdd=rdd.sample(withReplacement=False, fraction=0.001)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 46, "cell_type": "code", "source": "cantidad_estimacion=sampled_rdd.count()\nmedia_estimada=sampled_rdd.reduce(lambda a,b:a+b)/cantidad_estimacion\nprint(\"Media: %2.4f basada en una muestra de %d numeros\" % (media_estimada, cantidad_estimacion))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Media: 19.7093 basada en una muestra de 86 numeros"}], "metadata": {"collapsed": false}}, {"execution_count": 47, "cell_type": "code", "source": "rdd.reduce(lambda a,b:a+b)/rdd.count() #media real", "outputs": [{"output_type": "stream", "name": "stdout", "text": "20.9684"}], "metadata": {"collapsed": false}}, {"execution_count": 48, "cell_type": "code", "source": "sampled_rdd.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[31, 14, 39, 35, 1, 22, 28, 12, 5, 14, 6, 30, 11, 42, 37, 42, 30, 39, 13, 7, 10, 24, 15, 35, 5, 38, 40, 28, 30, 12, 0, 14, 1, 22, 42, 0, 41, 24, 11, 27, 22, 25, 13, 23, 3, 32, 27, 4, 33, 9, 22, 16, 12, 2, 1, 42, 25, 8, 4, 5, 17, 14, 41, 29, 31, 27, 11, 19, 33, 22, 11, 6, 14, 9, 6, 3, 31, 19, 22, 23, 10, 18, 4, 24, 37, 9]"}], "metadata": {"collapsed": false}}, {"execution_count": 49, "cell_type": "code", "source": "sampled_rdd.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[31, 14, 39, 35, 1, 22, 28, 12, 5, 14, 6, 30, 11, 42, 37, 42, 30, 39, 13, 7, 10, 24, 15, 35, 5, 38, 40, 28, 30, 12, 0, 14, 1, 22, 42, 0, 41, 24, 11, 27, 22, 25, 13, 23, 3, 32, 27, 4, 33, 9, 22, 16, 12, 2, 1, 42, 25, 8, 4, 5, 17, 14, 41, 29, 31, 27, 11, 19, 33, 22, 11, 6, 14, 9, 6, 3, 31, 19, 22, 23, 10, 18, 4, 24, 37, 9]"}], "metadata": {"collapsed": false}}, {"source": "# Acciones\n\n-Que son?", "cell_type": "markdown", "metadata": {}}, {"source": "## Acci\u00f3n Reduce", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "numeros=[1,2,3,4]\nrdd=sc.parallelize(numeros)\nrdd.reduce(lambda a,b: a+b)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "10"}], "metadata": {"collapsed": false}}, {"execution_count": 24, "cell_type": "code", "source": "numeros=[1,2,3,4]\nrdd=sc.parallelize(numeros)\ndef suma(a,b):\n    return a+b\n\nrdd.reduce(suma)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "10"}], "metadata": {"collapsed": false}}, {"execution_count": 25, "cell_type": "code", "source": "rdd.reduce(lambda acumulado, nuevo: str(acumulado)+\" <\"+str(nuevo)+\"> \")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "'1 <2>  <3 <4> > '"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicios:** \n* Encuentra el valor m\u00e1ximo del rdd generado a partir de los n\u00fameros del 1 al 100", "cell_type": "markdown", "metadata": {}}, {"execution_count": 80, "cell_type": "code", "source": "rdd_primeros_100_numeros.reduce(max)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "99"}], "metadata": {"collapsed": false}}, {"source": "* Encuentra el valor m\u00e1ximo divisible por 7 del rdd generado a partir de los n\u00fameros del 1 al 100", "cell_type": "markdown", "metadata": {}}, {"execution_count": 81, "cell_type": "code", "source": "rdd_primeros_100_numeros.filter(lambda numero: numero%7 == 0).reduce(max)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "98"}], "metadata": {"collapsed": false}}, {"source": "## Acci\u00f3n takeOrdered", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "rdd_1 = sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7])\nrdd_1.takeOrdered(6)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1, 2, 3, 4, 5, 6]"}], "metadata": {"collapsed": false}}, {"execution_count": 1, "cell_type": "code", "source": "rdd_1 = sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7])\nrdd_1.takeOrdered(6, key=lambda x:-x)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2061</td><td>application_1520861122949_1114</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-datasc.ilillgfpkp1uxkmlaiejqghkwe.fx.internal.cloudapp.net:8088/proxy/application_1520861122949_1114/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.7:30060/node/containerlogs/container_e59_1520861122949_1114_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n[10, 9, 7, 6, 5, 4]"}], "metadata": {"collapsed": false}}, {"source": "## Acci\u00f3n TakeSample", "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "cell_type": "code", "source": "lista=[\"ab\", \"cosa\", 4.3, 'ejemplo_1', 'ejemplo_2', 45, 85]\nrdd=sc.parallelize(lista)\nprint(rdd.takeSample(withReplacement=True, num=4, seed=1))\nprint(rdd.takeSample(withReplacement=False, num=2))\nprint(rdd.takeSample(withReplacement=True, num=4))\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[85, 'ejemplo_2', 'ejemplo_1', 'ejemplo_1']\n['ejemplo_1', 4.3]\n[4.3, 'ejemplo_1', 4.3, 4.3]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "# Pair RDDs\n\n- Es un rdd cuyos elementos son pares clave-valor, siendo la key el primer elemento y la clave el que ocupa la segunda posici\u00f3n\n\n\n- Se puede crear a partir de un rdd cualquiera usando dividiendo sus elementos en una clave y un valor a trav\u00e9s de un map del tipo: `map(lambda elemento: (elemento.split(\u2018,\u2019)[0], elemento.split(\u2018,\u2019)[1]))`\n\n\n- Son la base para las transformaciones por key, tales como groupByKey(), reduceByKey(), etc\u2026\n", "cell_type": "markdown", "metadata": {}}, {"source": "## Creaci\u00f3n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "PAIS=0\nMEDALLAS=1\nDEPORTE=2\n\n\ndatos=[(\"Espa\u00f1a\",1,\"Snowboard\"),(\"Canada\",2,\"Ski\"),(\"Espa\u00f1a\",1,\"Patinaje\"),\n       (\"Canada\",1,\"Biatl\u00f3n\"),(\"USA\",1,\"Salto\"),(\"Canada\",1,\"Bobsleigh\")]\n\nrdd=sc.parallelize(datos)\n\npair_rdd=rdd.map(lambda dato: (dato[PAIS], dato)) #mapeamos pais->(pais,medallas, deporte)\nimport pprint\n\npprint.pprint(pair_rdd.collect())\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('Espa\u00f1a', ('Espa\u00f1a', 1, 'Snowboard')),\n ('Canada', ('Canada', 2, 'Ski')),\n ('Espa\u00f1a', ('Espa\u00f1a', 1, 'Patinaje')),\n ('Canada', ('Canada', 1, 'Biatl\u00f3n')),\n ('USA', ('USA', 1, 'Salto')),\n ('Canada', ('Canada', 1, 'Bobsleigh'))]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "**Ejercicio:**  \nA partir del RDD que contiene n\u00fameros del 1 al 100, crea una PairRDD cuya clave sea el n\u00famero y cuyo valor si es par o no\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 86, "cell_type": "code", "source": "def par_o_no(numero):\n    if numero%2 == 0:\n        return 'par'\n    else:\n        return 'impar'\n    \n    \npair_rdd_par_impar = rdd_primeros_100_numeros.map(lambda numero: (str(numero), par_o_no(numero)))\npair_rdd_par_impar.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('1', 'impar'), ('2', 'par'), ('3', 'impar'), ('4', 'par'), ('5', 'impar'), ('6', 'par'), ('7', 'impar'), ('8', 'par'), ('9', 'impar'), ('10', 'par'), ('11', 'impar'), ('12', 'par'), ('13', 'impar'), ('14', 'par'), ('15', 'impar'), ('16', 'par'), ('17', 'impar'), ('18', 'par'), ('19', 'impar'), ('20', 'par'), ('21', 'impar'), ('22', 'par'), ('23', 'impar'), ('24', 'par'), ('25', 'impar'), ('26', 'par'), ('27', 'impar'), ('28', 'par'), ('29', 'impar'), ('30', 'par'), ('31', 'impar'), ('32', 'par'), ('33', 'impar'), ('34', 'par'), ('35', 'impar'), ('36', 'par'), ('37', 'impar'), ('38', 'par'), ('39', 'impar'), ('40', 'par'), ('41', 'impar'), ('42', 'par'), ('43', 'impar'), ('44', 'par'), ('45', 'impar'), ('46', 'par'), ('47', 'impar'), ('48', 'par'), ('49', 'impar'), ('50', 'par'), ('51', 'impar'), ('52', 'par'), ('53', 'impar'), ('54', 'par'), ('55', 'impar'), ('56', 'par'), ('57', 'impar'), ('58', 'par'), ('59', 'impar'), ('60', 'par'), ('61', 'impar'), ('62', 'par'), ('63', 'impar'), ('64', 'par'), ('65', 'impar'), ('66', 'par'), ('67', 'impar'), ('68', 'par'), ('69', 'impar'), ('70', 'par'), ('71', 'impar'), ('72', 'par'), ('73', 'impar'), ('74', 'par'), ('75', 'impar'), ('76', 'par'), ('77', 'impar'), ('78', 'par'), ('79', 'impar'), ('80', 'par'), ('81', 'impar'), ('82', 'par'), ('83', 'impar'), ('84', 'par'), ('85', 'impar'), ('86', 'par'), ('87', 'impar'), ('88', 'par'), ('89', 'impar'), ('90', 'par'), ('91', 'impar'), ('92', 'par'), ('93', 'impar'), ('94', 'par'), ('95', 'impar'), ('96', 'par'), ('97', 'impar'), ('98', 'par'), ('99', 'impar')]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**  \nA partir del RDD del ejercicio anterior, intercambia la clave y el valor\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 87, "cell_type": "code", "source": "pair_rdd_par_impar.map(lambda clave_valor: (clave_valor[1], clave_valor[0])).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('impar', '1'), ('par', '2'), ('impar', '3'), ('par', '4'), ('impar', '5'), ('par', '6'), ('impar', '7'), ('par', '8'), ('impar', '9'), ('par', '10'), ('impar', '11'), ('par', '12'), ('impar', '13'), ('par', '14'), ('impar', '15'), ('par', '16'), ('impar', '17'), ('par', '18'), ('impar', '19'), ('par', '20'), ('impar', '21'), ('par', '22'), ('impar', '23'), ('par', '24'), ('impar', '25'), ('par', '26'), ('impar', '27'), ('par', '28'), ('impar', '29'), ('par', '30'), ('impar', '31'), ('par', '32'), ('impar', '33'), ('par', '34'), ('impar', '35'), ('par', '36'), ('impar', '37'), ('par', '38'), ('impar', '39'), ('par', '40'), ('impar', '41'), ('par', '42'), ('impar', '43'), ('par', '44'), ('impar', '45'), ('par', '46'), ('impar', '47'), ('par', '48'), ('impar', '49'), ('par', '50'), ('impar', '51'), ('par', '52'), ('impar', '53'), ('par', '54'), ('impar', '55'), ('par', '56'), ('impar', '57'), ('par', '58'), ('impar', '59'), ('par', '60'), ('impar', '61'), ('par', '62'), ('impar', '63'), ('par', '64'), ('impar', '65'), ('par', '66'), ('impar', '67'), ('par', '68'), ('impar', '69'), ('par', '70'), ('impar', '71'), ('par', '72'), ('impar', '73'), ('par', '74'), ('impar', '75'), ('par', '76'), ('impar', '77'), ('par', '78'), ('impar', '79'), ('par', '80'), ('impar', '81'), ('par', '82'), ('impar', '83'), ('par', '84'), ('impar', '85'), ('par', '86'), ('impar', '87'), ('par', '88'), ('impar', '89'), ('par', '90'), ('impar', '91'), ('par', '92'), ('impar', '93'), ('par', '94'), ('impar', '95'), ('par', '96'), ('impar', '97'), ('par', '98'), ('impar', '99')]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n groupBy (RDD->PairRDD)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 98, "cell_type": "code", "source": "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\nresult = rdd.groupBy(lambda x: x % 2)\nresult.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(0, <pyspark.resultiterable.ResultIterable object at 0x7f92b12d8198>), (1, <pyspark.resultiterable.ResultIterable object at 0x7f92b12d84e0>)]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**  \nComo has visto, groupBy devuelve los values como iterables, utiliza map para que sean una lista", "cell_type": "markdown", "metadata": {}}, {"execution_count": 99, "cell_type": "code", "source": "result.map(lambda pair: (pair[0], list(pair[1]))).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(0, [2, 8]), (1, [1, 1, 3, 5])]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**  \nA partir del rdd generado de los 100 n\u00fameros. Agrupar los n\u00fameros \nseg\u00fan sean multiplos s\u00f3lo de 3, s\u00f3lo de 2, de ambos o de ninguno  \nPista: Usar una funci\u00f3n para simplificar la sentencia", "cell_type": "markdown", "metadata": {}}, {"execution_count": 107, "cell_type": "code", "source": "def multiplos(numero):\n    if numero%2 == 0:\n        if numero%3!=0:\n            return('multiplo de 2')\n        else:\n            return('multiplo de 2 y 3')\n    else:\n        if numero%3!=0:\n            return('multiplo de ninguno')\n        else:\n            return('multiplo de 3')\n\n\nrdd_primeros_100_numeros.groupBy(multiplos).map(lambda pair: (pair[0],list(pair[1]))).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('multiplo de 2 y 3', [6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96]), ('multiplo de ninguno', [1, 5, 7, 11, 13, 17, 19, 23, 25, 29, 31, 35, 37, 41, 43, 47, 49, 53, 55, 59, 61, 65, 67, 71, 73, 77, 79, 83, 85, 89, 91, 95, 97]), ('multiplo de 3', [3, 9, 15, 21, 27, 33, 39, 45, 51, 57, 63, 69, 75, 81, 87, 93, 99]), ('multiplo de 2', [2, 4, 8, 10, 14, 16, 20, 22, 26, 28, 32, 34, 38, 40, 44, 46, 50, 52, 56, 58, 62, 64, 68, 70, 74, 76, 80, 82, 86, 88, 92, 94, 98])]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**  \n- Listar las palabras del texto 'Viajes por Espa\u00f1a' que tienen m\u00e1s de 5 letras", "cell_type": "markdown", "metadata": {}}, {"execution_count": 120, "cell_type": "code", "source": "rdd_viajes_espana.flatMap(lambda linea: linea.split()).map(lambda palabra: re.sub('[^\\w]', '', palabra.lower()))\\\n.filter(lambda palabra: len(palabra)> 5).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['jinete', 'condici\u00f3n', 'cont\u00e1is', 'adem\u00e1s', 'cuatro', 'treinta', 'ten\u00e9is', '\u00faltimo', 'navalmoral', 'conocido', 'proporcione', 'caballo', 'pod\u00e9is', 'facil\u00edsimamente', 'primer', 'ofrecer\u00e1', 'reunidos', 'm\u00faltiples', 'exploraci\u00f3n', 'geogr\u00e1ficopintoresca', 'inter\u00e9s', 'excursi\u00f3n', 'historial', 'art\u00edstica', 'religiosa', 'complacencia', 'aquellas', 'romer\u00edas', 'verdaderamente', 'patri\u00f3ticas', 'cumplido', 'ufanan', 'alegran', 'todav\u00eda', 'respetan', 'tierra', 'pod\u00e9is', 'visitar', 'monasterio', 'suponemos', 'est\u00e1is', 'madrid', 'empezar\u00e9is', 'billete', 'berlina', 'interior', 'navalmoral', 'diligencia', 'c\u00e1ceres', 'diariamente', 'correo', 'carretera', 'general', 'ning\u00fan', 'paraje', 'peligrosa', 'pasar\u00e9is', 'sucesivamente', 'dehesa', 'carabancheles', 'artilleros', 'ten\u00edan', 'establecida', 'notable', 'escuela', 'pr\u00e1ctica', 'ventas', 'alcorc\u00f3n', 'alcorc\u00f3n', 'dij\u00e9ramos', 's\u00e8vres', 'actuales', 'madrile\u00f1os', 'm\u00f3stoles', 'acordar\u00e9is', '\u00f3rgano', 'c\u00e9lebre', 'alcalde', 'navalcarnero', 'principales', 'lagares', 'surten', 'pele\u00f3n', 'madrid', 'valmojado', 'mojado', 'terreno', 'arcilloso', 'retamar', 'abundante', 'fiebres', 'intermitentes', 'carbones', 'maqueda', 'todav\u00eda', 'monumental', 'cuanto', 'poderosa', 'antig\u00fcedad', 'romana', 'tiempos', 'nuestra', 'berenguela', 'olalla', 'patria', 'historiador', 'castro', 'predicador', 'crist\u00f3bal', 'fonseca', 'insignes', 'varones', 'literatos', 'amanecer', 'viaj\u00e9is', 'aconsejamos', 'primavera', 'encontrar\u00e9is', 'talavera', 'confirmada', 'supongo', 'recientemente', 'nombre', 'talavera', 'rep\u00fablica', 'federal', 'trayecto', 'hab\u00e9is', 'obscuridad', 'provey\u00e9ndoos', 'dormici\u00f3n', 'dormimiento', 'anta\u00f1o', 'evitar', 'confusiones', 'dormir', 'habr\u00e9is', 'perfectamente', 'esperan', 'grandes', 'hoteles', 'digamos', 'vuestra', 'romer\u00eda', 'llegar', 'talavera', 'detiene', 'chocolate', 'despertar\u00e9is', 'alguna', 'podr\u00e9is', 'muchas', 'buenas']"}], "metadata": {"collapsed": false}}, {"source": "- Agrupar las palabras del texto 'Viajes por Espa\u00f1a' seg\u00fan si tienen m\u00e1s de 5 letras o no", "cell_type": "markdown", "metadata": {}}, {"execution_count": 119, "cell_type": "code", "source": "def longitud_palabras(palabra):\n    if len(palabra)>5:\n        return 'mayor de 5 letras'\n    elif len(palabra)<5:\n        return 'menor que 5 letras'\n    else:\n        return '5 letras'\n\nrdd_viajes_espana.flatMap(lambda linea: linea.split()).map(lambda palabra: re.sub('[^\\w]', '', palabra.lower()))\\\n.groupBy(lambda palabra: longitud_palabras(palabra)).map(lambda pair: (pair[0], list(pair[1]))).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('5 letras', ['duros', 'sobra', 'alg\u00fan', 'hacer', 'viaje', 'orden', 'goces', 'grave', 'deber', 'sobre', 'yuste', 'tomar', 'hasta', 'calle', 'corte', 'siete', 'media', 'tarde', 'buena', 'donde', 'mismo', 'donde', 'tiene', 'valle', 'ocupa', 'santa', 'santa', 'alvar', 'g\u00f3mez', 'ambos', 'oto\u00f1o', 'reina', 'dicho', 'visto', 'causa', 'noche', 'haber', 'sue\u00f1o', 'dec\u00eda', 'entre', 'hecho', 'donde', 'coche', 'cosas']), ('mayor de 5 letras', ['jinete', 'condici\u00f3n', 'cont\u00e1is', 'adem\u00e1s', 'cuatro', 'treinta', 'ten\u00e9is', '\u00faltimo', 'navalmoral', 'conocido', 'proporcione', 'caballo', 'pod\u00e9is', 'facil\u00edsimamente', 'primer', 'ofrecer\u00e1', 'reunidos', 'm\u00faltiples', 'exploraci\u00f3n', 'geogr\u00e1ficopintoresca', 'inter\u00e9s', 'excursi\u00f3n', 'historial', 'art\u00edstica', 'religiosa', 'complacencia', 'aquellas', 'romer\u00edas', 'verdaderamente', 'patri\u00f3ticas', 'cumplido', 'ufanan', 'alegran', 'todav\u00eda', 'respetan', 'tierra', 'pod\u00e9is', 'visitar', 'monasterio', 'suponemos', 'est\u00e1is', 'madrid', 'empezar\u00e9is', 'billete', 'berlina', 'interior', 'navalmoral', 'diligencia', 'c\u00e1ceres', 'diariamente', 'correo', 'carretera', 'general', 'ning\u00fan', 'paraje', 'peligrosa', 'pasar\u00e9is', 'sucesivamente', 'dehesa', 'carabancheles', 'artilleros', 'ten\u00edan', 'establecida', 'notable', 'escuela', 'pr\u00e1ctica', 'ventas', 'alcorc\u00f3n', 'alcorc\u00f3n', 'dij\u00e9ramos', 's\u00e8vres', 'actuales', 'madrile\u00f1os', 'm\u00f3stoles', 'acordar\u00e9is', '\u00f3rgano', 'c\u00e9lebre', 'alcalde', 'navalcarnero', 'principales', 'lagares', 'surten', 'pele\u00f3n', 'madrid', 'valmojado', 'mojado', 'terreno', 'arcilloso', 'retamar', 'abundante', 'fiebres', 'intermitentes', 'carbones', 'maqueda', 'todav\u00eda', 'monumental', 'cuanto', 'poderosa', 'antig\u00fcedad', 'romana', 'tiempos', 'nuestra', 'berenguela', 'olalla', 'patria', 'historiador', 'castro', 'predicador', 'crist\u00f3bal', 'fonseca', 'insignes', 'varones', 'literatos', 'amanecer', 'viaj\u00e9is', 'aconsejamos', 'primavera', 'encontrar\u00e9is', 'talavera', 'confirmada', 'supongo', 'recientemente', 'nombre', 'talavera', 'rep\u00fablica', 'federal', 'trayecto', 'hab\u00e9is', 'obscuridad', 'provey\u00e9ndoos', 'dormici\u00f3n', 'dormimiento', 'anta\u00f1o', 'evitar', 'confusiones', 'dormir', 'habr\u00e9is', 'perfectamente', 'esperan', 'grandes', 'hoteles', 'digamos', 'vuestra', 'romer\u00eda', 'llegar', 'talavera', 'detiene', 'chocolate', 'despertar\u00e9is', 'alguna', 'podr\u00e9is', 'muchas', 'buenas']), ('menor que 5 letras', ['si', 'sois', 'algo', 'sine', 'qua', 'non', 'si', 'con', 'd\u00edas', 'y', 'de', 'y', 'por', 'en', 'de', 'la', 'mata', 'que', 'os', 'y', 'gu\u00eda', 'un', 'de', 'que', 'os', 'los', 'de', 'una', 'el', 'de', 'una', 'y', 'y', 'la', 'de', 'que', 'como', 'todo', 'y', 'el', 'alma', 'de', 'los', 'que', 'algo', 'la', 'en', 'suma', 'el', 'de', 'para', 'ello', 'que', 'en', 'por', 'un', 'de', 'o', 'de', 'de', 'la', 'mata', 'en', 'la', 'de', 'que', 'sale', 'de', 'la', 'del', 'de', '\u00e9sta', 'que', 'fu\u00e9', 'a', 'las', 'y', 'de', 'la', 'la', 'es', 'por', 'lo', 'y', 'en', 'por', 'la', 'de', 'los', 'los', 'su', 'muy', 'por', 'las', 'de', 'y', 'por', 'que', 'es', 'como', 'si', 'por', 'el', 'de', 'los', 'por', 'os', 'de', 'su', 'y', 'de', 'su', 'del', 'a\u00f1o', 'de', '1808', 'por', 'uno', 'de', 'los', 'que', 'de', 'a', 'por', 'que', 'nada', 'de', 'ni', 'de', 'pues', 'un', 'muy', 'alto', 'y', 'por', 'cruz', 'del', 'en', 'y', 'en', 'por', 'hoy', 'en', 'la', 'y', 'en', 'de', 'do\u00f1a', 'y', 'en', 'fin', 'por', 'del', 'de', 'y', 'del', 'y', 'con', 'lo', 'cual', 'al', 'dado', 'que', 'como', 'os', 'lo', 'en', '\u00f3', 'en', 'os', 'en', 'de', 'la', 'con', 'el', 'de', 'de', 'la', 'se', 'est\u00e1', 'que', 'en', 'todo', 'este', 'no', 'casi', 'nada', 'a', 'de', 'la', 'de', 'la', 'y', 'de', 'ido', 'de', 'o', 'bien', 'de', 'o', 'como', 'se', 'para', 'la', 'gana', 'y', 'el', 'acto', 'de', 'y', 'en', 'ello', 'pues', 'no', 'os', 'que', 'en', 'toda', 'pero', 'al', 'a', 'se', 'el', 'una', 'hora', 'y', 'se', 'toma', 'sin', 'duda', 'y', 'ver', 'al', 'paso', 'y', 'muy'])]"}], "metadata": {"collapsed": false}}, {"source": "- Intenta realizar este ejercicio repitiendo el menor c\u00f3digo posible", "cell_type": "markdown", "metadata": {}}, {"execution_count": 122, "cell_type": "code", "source": "rdd_palabras_alfanum_viajes_espana = rdd_viajes_espana.flatMap(lambda linea: linea.split())\\\n.map(lambda palabra: re.sub('[^\\w]', '', palabra.lower()))\nprint('Primer apartado:\\n')\nprint(rdd_palabras_alfanum_viajes_espana.filter(lambda palabra: len(palabra)> 5).collect())\nprint('\\nSegundo apartado:\\n')\nprint(rdd_palabras_alfanum_viajes_espana.groupBy(lambda palabra: longitud_palabras(palabra))\\\n      .map(lambda pair: (pair[0], list(pair[1]))).collect())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Primer apartado:\n\n['jinete', 'condici\u00f3n', 'cont\u00e1is', 'adem\u00e1s', 'cuatro', 'treinta', 'ten\u00e9is', '\u00faltimo', 'navalmoral', 'conocido', 'proporcione', 'caballo', 'pod\u00e9is', 'facil\u00edsimamente', 'primer', 'ofrecer\u00e1', 'reunidos', 'm\u00faltiples', 'exploraci\u00f3n', 'geogr\u00e1ficopintoresca', 'inter\u00e9s', 'excursi\u00f3n', 'historial', 'art\u00edstica', 'religiosa', 'complacencia', 'aquellas', 'romer\u00edas', 'verdaderamente', 'patri\u00f3ticas', 'cumplido', 'ufanan', 'alegran', 'todav\u00eda', 'respetan', 'tierra', 'pod\u00e9is', 'visitar', 'monasterio', 'suponemos', 'est\u00e1is', 'madrid', 'empezar\u00e9is', 'billete', 'berlina', 'interior', 'navalmoral', 'diligencia', 'c\u00e1ceres', 'diariamente', 'correo', 'carretera', 'general', 'ning\u00fan', 'paraje', 'peligrosa', 'pasar\u00e9is', 'sucesivamente', 'dehesa', 'carabancheles', 'artilleros', 'ten\u00edan', 'establecida', 'notable', 'escuela', 'pr\u00e1ctica', 'ventas', 'alcorc\u00f3n', 'alcorc\u00f3n', 'dij\u00e9ramos', 's\u00e8vres', 'actuales', 'madrile\u00f1os', 'm\u00f3stoles', 'acordar\u00e9is', '\u00f3rgano', 'c\u00e9lebre', 'alcalde', 'navalcarnero', 'principales', 'lagares', 'surten', 'pele\u00f3n', 'madrid', 'valmojado', 'mojado', 'terreno', 'arcilloso', 'retamar', 'abundante', 'fiebres', 'intermitentes', 'carbones', 'maqueda', 'todav\u00eda', 'monumental', 'cuanto', 'poderosa', 'antig\u00fcedad', 'romana', 'tiempos', 'nuestra', 'berenguela', 'olalla', 'patria', 'historiador', 'castro', 'predicador', 'crist\u00f3bal', 'fonseca', 'insignes', 'varones', 'literatos', 'amanecer', 'viaj\u00e9is', 'aconsejamos', 'primavera', 'encontrar\u00e9is', 'talavera', 'confirmada', 'supongo', 'recientemente', 'nombre', 'talavera', 'rep\u00fablica', 'federal', 'trayecto', 'hab\u00e9is', 'obscuridad', 'provey\u00e9ndoos', 'dormici\u00f3n', 'dormimiento', 'anta\u00f1o', 'evitar', 'confusiones', 'dormir', 'habr\u00e9is', 'perfectamente', 'esperan', 'grandes', 'hoteles', 'digamos', 'vuestra', 'romer\u00eda', 'llegar', 'talavera', 'detiene', 'chocolate', 'despertar\u00e9is', 'alguna', 'podr\u00e9is', 'muchas', 'buenas']\n\nSegundo apartado:\n\n[('5 letras', ['duros', 'sobra', 'alg\u00fan', 'hacer', 'viaje', 'orden', 'goces', 'grave', 'deber', 'sobre', 'yuste', 'tomar', 'hasta', 'calle', 'corte', 'siete', 'media', 'tarde', 'buena', 'donde', 'mismo', 'donde', 'tiene', 'valle', 'ocupa', 'santa', 'santa', 'alvar', 'g\u00f3mez', 'ambos', 'oto\u00f1o', 'reina', 'dicho', 'visto', 'causa', 'noche', 'haber', 'sue\u00f1o', 'dec\u00eda', 'entre', 'hecho', 'donde', 'coche', 'cosas']), ('mayor de 5 letras', ['jinete', 'condici\u00f3n', 'cont\u00e1is', 'adem\u00e1s', 'cuatro', 'treinta', 'ten\u00e9is', '\u00faltimo', 'navalmoral', 'conocido', 'proporcione', 'caballo', 'pod\u00e9is', 'facil\u00edsimamente', 'primer', 'ofrecer\u00e1', 'reunidos', 'm\u00faltiples', 'exploraci\u00f3n', 'geogr\u00e1ficopintoresca', 'inter\u00e9s', 'excursi\u00f3n', 'historial', 'art\u00edstica', 'religiosa', 'complacencia', 'aquellas', 'romer\u00edas', 'verdaderamente', 'patri\u00f3ticas', 'cumplido', 'ufanan', 'alegran', 'todav\u00eda', 'respetan', 'tierra', 'pod\u00e9is', 'visitar', 'monasterio', 'suponemos', 'est\u00e1is', 'madrid', 'empezar\u00e9is', 'billete', 'berlina', 'interior', 'navalmoral', 'diligencia', 'c\u00e1ceres', 'diariamente', 'correo', 'carretera', 'general', 'ning\u00fan', 'paraje', 'peligrosa', 'pasar\u00e9is', 'sucesivamente', 'dehesa', 'carabancheles', 'artilleros', 'ten\u00edan', 'establecida', 'notable', 'escuela', 'pr\u00e1ctica', 'ventas', 'alcorc\u00f3n', 'alcorc\u00f3n', 'dij\u00e9ramos', 's\u00e8vres', 'actuales', 'madrile\u00f1os', 'm\u00f3stoles', 'acordar\u00e9is', '\u00f3rgano', 'c\u00e9lebre', 'alcalde', 'navalcarnero', 'principales', 'lagares', 'surten', 'pele\u00f3n', 'madrid', 'valmojado', 'mojado', 'terreno', 'arcilloso', 'retamar', 'abundante', 'fiebres', 'intermitentes', 'carbones', 'maqueda', 'todav\u00eda', 'monumental', 'cuanto', 'poderosa', 'antig\u00fcedad', 'romana', 'tiempos', 'nuestra', 'berenguela', 'olalla', 'patria', 'historiador', 'castro', 'predicador', 'crist\u00f3bal', 'fonseca', 'insignes', 'varones', 'literatos', 'amanecer', 'viaj\u00e9is', 'aconsejamos', 'primavera', 'encontrar\u00e9is', 'talavera', 'confirmada', 'supongo', 'recientemente', 'nombre', 'talavera', 'rep\u00fablica', 'federal', 'trayecto', 'hab\u00e9is', 'obscuridad', 'provey\u00e9ndoos', 'dormici\u00f3n', 'dormimiento', 'anta\u00f1o', 'evitar', 'confusiones', 'dormir', 'habr\u00e9is', 'perfectamente', 'esperan', 'grandes', 'hoteles', 'digamos', 'vuestra', 'romer\u00eda', 'llegar', 'talavera', 'detiene', 'chocolate', 'despertar\u00e9is', 'alguna', 'podr\u00e9is', 'muchas', 'buenas']), ('menor que 5 letras', ['si', 'sois', 'algo', 'sine', 'qua', 'non', 'si', 'con', 'd\u00edas', 'y', 'de', 'y', 'por', 'en', 'de', 'la', 'mata', 'que', 'os', 'y', 'gu\u00eda', 'un', 'de', 'que', 'os', 'los', 'de', 'una', 'el', 'de', 'una', 'y', 'y', 'la', 'de', 'que', 'como', 'todo', 'y', 'el', 'alma', 'de', 'los', 'que', 'algo', 'la', 'en', 'suma', 'el', 'de', 'para', 'ello', 'que', 'en', 'por', 'un', 'de', 'o', 'de', 'de', 'la', 'mata', 'en', 'la', 'de', 'que', 'sale', 'de', 'la', 'del', 'de', '\u00e9sta', 'que', 'fu\u00e9', 'a', 'las', 'y', 'de', 'la', 'la', 'es', 'por', 'lo', 'y', 'en', 'por', 'la', 'de', 'los', 'los', 'su', 'muy', 'por', 'las', 'de', 'y', 'por', 'que', 'es', 'como', 'si', 'por', 'el', 'de', 'los', 'por', 'os', 'de', 'su', 'y', 'de', 'su', 'del', 'a\u00f1o', 'de', '1808', 'por', 'uno', 'de', 'los', 'que', 'de', 'a', 'por', 'que', 'nada', 'de', 'ni', 'de', 'pues', 'un', 'muy', 'alto', 'y', 'por', 'cruz', 'del', 'en', 'y', 'en', 'por', 'hoy', 'en', 'la', 'y', 'en', 'de', 'do\u00f1a', 'y', 'en', 'fin', 'por', 'del', 'de', 'y', 'del', 'y', 'con', 'lo', 'cual', 'al', 'dado', 'que', 'como', 'os', 'lo', 'en', '\u00f3', 'en', 'os', 'en', 'de', 'la', 'con', 'el', 'de', 'de', 'la', 'se', 'est\u00e1', 'que', 'en', 'todo', 'este', 'no', 'casi', 'nada', 'a', 'de', 'la', 'de', 'la', 'y', 'de', 'ido', 'de', 'o', 'bien', 'de', 'o', 'como', 'se', 'para', 'la', 'gana', 'y', 'el', 'acto', 'de', 'y', 'en', 'ello', 'pues', 'no', 'os', 'que', 'en', 'toda', 'pero', 'al', 'a', 'se', 'el', 'una', 'hora', 'y', 'se', 'toma', 'sin', 'duda', 'y', 'ver', 'al', 'paso', 'y', 'muy'])]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n ReduceByKey (K,V)=>(K,V)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 123, "cell_type": "code", "source": "rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\nrdd2 = rdd.reduceByKey(lambda e, i: e+i)\nrdd2.collect()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(0, 13), (2, 6), (1, 3), (3, 16)]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 147, "cell_type": "code", "source": "PAIS=0\nMEDALLAS=1\nDEPORTE=2\n\nrdd=sc.parallelize([\n\n('Espa\u00f1a', 1, 'Snowboard'),\n('Canada', 2, 'Ski'),\n('Espa\u00f1a', 1, 'Patinaje'),\n('Canada', 1, 'Biatl\u00f3n'),\n('USA', 1, 'Salto'),\n('Canada', 1, 'Bobsleigh'),\n        \n])\npair_rdd=rdd.map(lambda terna: (terna[0], terna))\npair_rdd.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('Espa\u00f1a', ('Espa\u00f1a', 1, 'Snowboard')), ('Canada', ('Canada', 2, 'Ski')), ('Espa\u00f1a', ('Espa\u00f1a', 1, 'Patinaje')), ('Canada', ('Canada', 1, 'Biatl\u00f3n')), ('USA', ('USA', 1, 'Salto')), ('Canada', ('Canada', 1, 'Bobsleigh'))]"}], "metadata": {"collapsed": false}}, {"source": "**Ejemplo**: queremos tener en una manera de relacionar pa\u00eds-->num_medallas", "cell_type": "markdown", "metadata": {}}, {"execution_count": 148, "cell_type": "code", "source": "import pprint\npprint.pprint(pair_rdd.reduceByKey(lambda value1, value2: value1+value2).collect())\n# Para cada pa\u00eds se concatenan sus tuplas - es un ejemplo, pero no es algo que t\u00edpicamente necesitemos", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('USA', ('USA', 1, 'Salto')),\n ('Espa\u00f1a', ('Espa\u00f1a', 1, 'Snowboard', 'Espa\u00f1a', 1, 'Patinaje')),\n ('Canada',\n  ('Canada', 2, 'Ski', 'Canada', 1, 'Biatl\u00f3n', 'Canada', 1, 'Bobsleigh'))]"}], "metadata": {"collapsed": false}}, {"execution_count": 149, "cell_type": "code", "source": "pprint.pprint(pair_rdd.reduceByKey(lambda value1, value2: value1[MEDALLAS]+value2[MEDALLAS]).collect())\n# No funciona ya que sumamos ints con tuplas", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 165.0 failed 4 times, most recent failure: Lost task 1.3 in stage 165.0 (TID 415, 10.0.0.7, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 345, in func\n    return f(iterator)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1835, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<stdin>\", line 1, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 345, in func\n    return f(iterator)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1835, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<stdin>\", line 1, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 808, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 165.0 failed 4 times, most recent failure: Lost task 1.3 in stage 165.0 (TID 415, 10.0.0.7, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 345, in func\n    return f(iterator)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1835, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<stdin>\", line 1, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 345, in func\n    return f(iterator)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1835, in _mergeCombiners\n    merger.mergeCombiners(iterator)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/shuffle.py\", line 272, in mergeCombiners\n    d[k] = comb(d[k], v) if k in d else v\n  File \"<stdin>\", line 1, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n"}], "metadata": {"collapsed": true}}, {"execution_count": 150, "cell_type": "code", "source": "#recordamos el \nc = pair_rdd.collect()\nfor i in c:\n    print(i)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "('Espa\u00f1a', ('Espa\u00f1a', 1, 'Snowboard'))\n('Canada', ('Canada', 2, 'Ski'))\n('Espa\u00f1a', ('Espa\u00f1a', 1, 'Patinaje'))\n('Canada', ('Canada', 1, 'Biatl\u00f3n'))\n('USA', ('USA', 1, 'Salto'))\n('Canada', ('Canada', 1, 'Bobsleigh'))"}], "metadata": {"collapsed": false}}, {"execution_count": 151, "cell_type": "code", "source": "pair_rdd2=pair_rdd.map(lambda pair: (pair[0], pair[1][MEDALLAS]))\npprint.pprint(pair_rdd2.collect())\n# Modificamos el pair_rdd para que admita nil-ops", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('Espa\u00f1a', 1),\n ('Canada', 2),\n ('Espa\u00f1a', 1),\n ('Canada', 1),\n ('USA', 1),\n ('Canada', 1)]"}], "metadata": {"collapsed": false}}, {"execution_count": 152, "cell_type": "code", "source": "pair_rdd2.reduceByKey(lambda value1, value2: value1+value2).collect()\n# Y ahora s\u00ed que tenemos la suma del medallero", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('USA', 1), ('Espa\u00f1a', 2), ('Canada', 4)]"}], "metadata": {"collapsed": false}}, {"execution_count": 153, "cell_type": "code", "source": "pair_rdd2.reduceByKey(lambda value1, value2: value1+value2).map(lambda x: {x[0]:x[1]}).collect()\n#La cambiamos para que sean diccionarios, en vez de tuplas", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[{'USA': 1}, {'Espa\u00f1a': 2}, {'Canada': 4}]"}], "metadata": {"collapsed": false}}, {"execution_count": 154, "cell_type": "code", "source": "def combina_diccionarios(dict1,dict2):\n    '''\n    >>> newdict=combina_diccionarios({'k1':'v1'},{'k2':'v2'})\n    {'k1': 'v1', 'k2': 'v2'}\n    '''\n    return {**dict1, **dict2}", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 155, "cell_type": "code", "source": "pair_rdd2.reduceByKey(lambda value1, value2: value1+value2).map(lambda x: {x[0]:x[1]}).reduce(combina_diccionarios)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "{'USA': 1, 'Espa\u00f1a': 2, 'Canada': 4}"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": " ", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "## Transformaci\u00f3n AggregateByKey (K,V) => (K,U)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 156, "cell_type": "code", "source": "pair_rdd.collect()\n#Recordemos", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('Espa\u00f1a', ('Espa\u00f1a', 1, 'Snowboard')), ('Canada', ('Canada', 2, 'Ski')), ('Espa\u00f1a', ('Espa\u00f1a', 1, 'Patinaje')), ('Canada', ('Canada', 1, 'Biatl\u00f3n')), ('USA', ('USA', 1, 'Salto')), ('Canada', ('Canada', 1, 'Bobsleigh'))]"}], "metadata": {"collapsed": false}}, {"execution_count": 165, "cell_type": "code", "source": "pair_rdd.aggregateByKey(\n    zeroValue=0,                                    # Valor inicial\n    seqFunc=lambda acc,rdd: acc+rdd[MEDALLAS],      # C\u00f3mo a\u00f1adir una rdd al acumulador?\n    combFunc=lambda x,y: x+y                        # C\u00f3mo agrupar el resultado de todas las seqFunc(si hay m\u00e1s de una)\n).collect()            ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('USA', 1), ('Espa\u00f1a', 2), ('Canada', 4)]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 166, "cell_type": "code", "source": "pair_rdd.aggregateByKey(\n    zeroValue=\"\",                                            # Valor inicial\n    seqFunc=lambda acc,rdd: acc+\",\"+rdd[DEPORTE],              # C\u00f3mo a\u00f1adir una rdd al acumulador?\n    combFunc=lambda x,y: x+  \",\"  +  y  # C\u00f3mo agrupar el resultado de todas las seqFunc (si hay mas de una)\n).collect()                                                  ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('USA', ',Salto'), ('Espa\u00f1a', ',Snowboard,Patinaje'), ('Canada', ',Ski,,Biatl\u00f3n,Bobsleigh')]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n CombineByKey (Un _AggregateByKey_ avanzado)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 169, "cell_type": "code", "source": "pair_rdd.combineByKey(\n    createCombiner=lambda v: v[DEPORTE],              # Dada una RDD crear un valor a partir de ella\n    mergeValue=lambda c,v: v[DEPORTE]+\". \"+c,         # Dada una RDD agregar su valor al valor ya calculado\n    mergeCombiners=lambda c1,c2: c1+\"; \"+c2           # Agregar dos valores calculados\n).collect()   ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('USA', 'Salto'), ('Espa\u00f1a', 'Patinaje. Snowboard'), ('Canada', 'Ski; Bobsleigh. Biatl\u00f3n')]"}], "metadata": {"collapsed": false}}, {"source": "## Transformaci\u00f3n join", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "rdd1 = sc.parallelize([('k1','a'),('k1','b'),('k1','c'),('k2','d'),('k2','e'),('k3','f'),('k3','g')])\nrdd2 = sc.parallelize([('k1','o'),('k1','p'),('k2','q'),('k2','r'),('k2','s'),('k5','t')])\n\nrdd1.join(rdd2).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('k2', ('d', 's')), ('k2', ('d', 'r')), ('k2', ('d', 'q')), ('k2', ('e', 's')), ('k2', ('e', 'r')), ('k2', ('e', 'q')), ('k1', ('a', 'o')), ('k1', ('a', 'p')), ('k1', ('b', 'o')), ('k1', ('b', 'p')), ('k1', ('c', 'o')), ('k1', ('c', 'p'))]"}], "metadata": {"collapsed": false}}, {"source": "**Ejercicio:**\nTenemos:\n- 2 equipos de futbol senior de Baleares: \u201dIbiza FC\u201d, \u201dMallorca FC\u201d\n- 1 equipo de futbol junior de Baleares: \"Menorca Junior FC\"\n- 2 equipos de futbol senior de Canarias: \"Tenerife FC\", \"Gomera FC\"\n- 2 equipos de futbol junior de Canarias: \"Fuerteventura Junior FC\" y \"Lanzarote Junior FC\"\n\nQueremos hacer una competici\u00f3n de tal manera que todos los equipos senior de un archipi\u00e9lago se enfrenten con todos los equipos senior del otro archipielago, y an\u00e1logamente para los equipos junior\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 171, "cell_type": "code", "source": "rdd_b=sc.parallelize([(\"Senior\",\"Ibiza FC\"), (\"Senior\", \"Mallorca FC\"), (\"Junior\", \"Menorca Junior FC\")])\nrdd_c=sc.parallelize([(\"Senior\",\"Tenerife FC\"), (\"Senior\", \"Gomera FC\"), (\"Junior\", \"Fuerteventura Junior FC\"),(\"Junior\", \"Lanzarote Junior FC\")])                      \n\nrdd_b.join(rdd_c).map(lambda cruce: cruce[1]).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('Ibiza FC', 'Tenerife FC'), ('Ibiza FC', 'Gomera FC'), ('Mallorca FC', 'Tenerife FC'), ('Mallorca FC', 'Gomera FC'), ('Menorca Junior FC', 'Fuerteventura Junior FC'), ('Menorca Junior FC', 'Lanzarote Junior FC')]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "# Un ejemplo cl\u00e1sico, el conteo de palabras", "cell_type": "markdown", "metadata": {}}, {"execution_count": 177, "cell_type": "code", "source": "path = \"adl://barcelodatalake.azuredatalakestore.net/pruebas/Nacho/quijote-2parrafos.txt\"\nlines = sc.textFile(path)\ncounts = lines.flatMap(lambda x: x.split(' ')).map(lambda w: ''.join([c for c in w if c.isalnum()])).map(lambda palabra: (palabra, 1)).reduceByKey(lambda x,y:x+y)\n\noutput=counts.collect()\noutput = sorted(output, key=lambda o:o[1], reverse=True)\nfor palabra,conteo in output:\n\n    print(palabra,\"-->\",conteo)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "de --> 38\nque --> 21\ny --> 16\nlos --> 13\nla --> 12\nen --> 8\nsu --> 7\na --> 7\ncon --> 6\nse --> 6\nm\u00e1s --> 5\nraz\u00f3n --> 4\nel --> 4\nno --> 4\nun --> 4\nlas --> 4\nhidalgo --> 3\nesto --> 3\nvuestra --> 3\nleer --> 3\npara --> 2\ntodos --> 2\nlibros --> 2\ncomo --> 2\ncaza --> 2\nle --> 2\nhacienda --> 2\nmuchas --> 2\ncaballer\u00edas --> 2\nos --> 2\ndel --> 2\nmi --> 2\npunto --> 2\nlo --> 2\nas\u00ed --> 2\nparec\u00edan --> 2\nY --> 2\nuna --> 2\nnuestro --> 2\nroc\u00edn --> 2\nllegaba --> 2\ncuando --> 2\ncasa --> 2\npartes --> 2\n --> 1\ntambi\u00e9n --> 1\ngrandeza --> 1\nlleg\u00f3 --> 1\neste --> 1\na\u00f1adidura --> 1\nratos --> 1\nFrisaba --> 1\nseco --> 1\nenjuto --> 1\nMancha --> 1\na\u00f1os --> 1\nningunos --> 1\npues --> 1\nresto --> 1\nenflaquece --> 1\nhanegas --> 1\nperlas --> 1\nvelludo --> 1\ncomprar --> 1\nentresemana --> 1\nPero --> 1\ncarnes --> 1\nhonraba --> 1\nmerecedora --> 1\nviv\u00eda --> 1\ndeste --> 1\nrequiebros --> 1\nantigua --> 1\ncielos --> 1\nQuesada --> 1\ndaba --> 1\nalg\u00fan --> 1\nhallaba --> 1\naltos --> 1\ncuriosidad --> 1\nestaba --> 1\nmozo --> 1\nsaber --> 1\npasaba --> 1\nvaca --> 1\nUna --> 1\nhacen --> 1\ndesatino --> 1\ntres --> 1\ncuyo --> 1\nsobrina --> 1\ngran --> 1\ncuantos --> 1\nentender --> 1\ntomaba --> 1\nveros\u00edmiles --> 1\nensillaba --> 1\naun --> 1\nconclu\u00edan --> 1\nsuyas --> 1\nsayo --> 1\nvellor\u00ed --> 1\nfiestas --> 1\nmerecimiento --> 1\nalguna --> 1\nFeliciano --> 1\nautores --> 1\npalomino --> 1\nnombre --> 1\ns\u00e1bados --> 1\nmadrugador --> 1\nrecia --> 1\nSilva --> 1\nastillero --> 1\ncaso --> 1\nporque --> 1\nquejo --> 1\nsus --> 1\nadministraci\u00f3n --> 1\ngusto --> 1\nhay --> 1\ncomplexi\u00f3n --> 1\ndecir --> 1\ncampo --> 1\ndivinamente --> 1\ntanto --> 1\nfino --> 1\ndiferencia --> 1\nbasta --> 1\nsobredicho --> 1\nQuijada --> 1\nplaza --> 1\nmucho --> 1\nrazones --> 1\nfortifican --> 1\nimporta --> 1\nbien --> 1\nrostro --> 1\ncarnero --> 1\nEn --> 1\ncuarenta --> 1\ntierra --> 1\nverdad --> 1\nmesmo --> 1\ncalzas --> 1\nnoches --> 1\nten\u00eda --> 1\neran --> 1\nTen\u00eda --> 1\nsobrenombre --> 1\nsinraz\u00f3n --> 1\nsalpic\u00f3n --> 1\ncuento --> 1\nnarraci\u00f3n --> 1\nEs --> 1\ntan --> 1\nduelos --> 1\nedad --> 1\nLa --> 1\nejercicio --> 1\nvendi\u00f3 --> 1\nentricadas --> 1\nllamaba --> 1\nocioso --> 1\ndesaf\u00edos --> 1\nle\u00eda --> 1\nQuejana --> 1\nescriben --> 1\nprosa --> 1\nlugar --> 1\nme --> 1\ndonde --> 1\nlanza --> 1\nveinte --> 1\ncompuso --> 1\ntanta --> 1\nlantejas --> 1\npantuflos --> 1\nolvid\u00f3 --> 1\nacordarme --> 1\npodadera --> 1\nflaco --> 1\naunque --> 1\npoco --> 1\ncorredor --> 1\nsalga --> 1\ntal --> 1\nalgo --> 1\nsembradura --> 1\nhaber --> 1\ncasi --> 1\nquebrantos --> 1\nadarga --> 1\namigo --> 1\na\u00f1o --> 1\npudo --> 1\naquellas --> 1\nera --> 1\nescrito --> 1\ndomingos --> 1\npor --> 1\nhace --> 1\nfermosura --> 1\nd\u00e9l --> 1\ndellos --> 1\nama --> 1\ncartas --> 1\nllev\u00f3 --> 1\nha --> 1\nconjeturas --> 1\ndivinidad --> 1\ntiempo --> 1\nquiero --> 1\nmerece --> 1\no --> 1\ntodo --> 1\nmanera --> 1\ncincuenta --> 1\nfamoso --> 1\ndeja --> 1\naquellos --> 1\nestrellas --> 1\nd\u00edas --> 1\nviernes --> 1\nolla --> 1\nvelarte --> 1\nconsum\u00edan --> 1\nclaridad --> 1\ngalgo --> 1\nQuieren --> 1\ndella --> 1\nafici\u00f3n --> 1\nEl --> 1"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "**Ejercicio:**\nMejora el ejemplo anterior, siguiendo buenas pr\u00e1cticas de programaci\u00f3n (por ejemplo evitando las lambdas)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 185, "cell_type": "code", "source": "path = \"adl://barcelodatalake.azuredatalakestore.net/pruebas/Nacho/quijote-2parrafos.txt\"\nlines = sc.textFile(path)\n\ndef separador_y_limpieza(linea):\n    palabras = linea.split()\n    palabras = [c for c in palabras if c.isalnum()]\n    return palabras\n\ncounts = lines.flatMap(lambda linea: separador_y_limpieza(linea)).map(lambda palabra: (palabra, 1)).reduceByKey(lambda x,y:x+y)\n\noutput=counts.collect()\noutput = sorted(output, key=lambda o:o[1], reverse=True)\nfor palabra,conteo in output:\n\n    print(palabra,\"-->\",conteo)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "de --> 38\nque --> 21\ny --> 16\nlos --> 12\nla --> 12\nen --> 8\nsu --> 7\na --> 7\ncon --> 6\nse --> 6\nm\u00e1s --> 5\nno --> 4\nraz\u00f3n --> 4\nel --> 4\nun --> 4\nlas --> 4\nvuestra --> 3\nhidalgo --> 2\npara --> 2\nos --> 2\nesto --> 2\nlibros --> 2\ncomo --> 2\nle --> 2\nmuchas --> 2\ndel --> 2\nmi --> 2\npunto --> 2\nlo --> 2\nparec\u00edan --> 2\ncasa --> 2\npartes --> 2\nnuestro --> 2\nY --> 2\nuna --> 2\ncuando --> 2\nleer --> 2\nllegaba --> 2\nroc\u00edn --> 2\ntambi\u00e9n --> 1\nlleg\u00f3 --> 1\nningunos --> 1\nalguna --> 1\nconclu\u00edan --> 1\nFrisaba --> 1\ndivinamente --> 1\na\u00f1adidura --> 1\nenjuto --> 1\ndiferencia --> 1\nsuyas --> 1\nresto --> 1\ntodos --> 1\nratos --> 1\nhanegas --> 1\nvelludo --> 1\nentresemana --> 1\nPero --> 1\nmerecedora --> 1\nviv\u00eda --> 1\ndeste --> 1\naun --> 1\nestaba --> 1\ndaba --> 1\ncaballer\u00edas --> 1\nalg\u00fan --> 1\nhallaba --> 1\naltos --> 1\ncielos --> 1\nmozo --> 1\ndesatino --> 1\nrequiebros --> 1\nUna --> 1\nhacen --> 1\nsaber --> 1\ntres --> 1\ngran --> 1\nadministraci\u00f3n --> 1\nhay --> 1\ncuantos --> 1\nrazones --> 1\nvaca --> 1\nhonraba --> 1\nnombre --> 1\nsayo --> 1\nvellor\u00ed --> 1\nmerecimiento --> 1\nFeliciano --> 1\nautores --> 1\npalomino --> 1\nmadrugador --> 1\npasaba --> 1\ncaso --> 1\nquejo --> 1\nsus --> 1\ncuriosidad --> 1\ncuyo --> 1\nsobrina --> 1\nensillaba --> 1\ncomplexi\u00f3n --> 1\ndecir --> 1\ncampo --> 1\ntanto --> 1\nsobredicho --> 1\neste --> 1\nseco --> 1\nas\u00ed --> 1\nmucho --> 1\nentender --> 1\ncomprar --> 1\nimporta --> 1\nbien --> 1\ntomaba --> 1\nEn --> 1\ntierra --> 1\nporque --> 1\ncalzas --> 1\nbasta --> 1\nten\u00eda --> 1\neran --> 1\nTen\u00eda --> 1\nsobrenombre --> 1\nhaber --> 1\npudo --> 1\ncasi --> 1\nadarga --> 1\namigo --> 1\nsinraz\u00f3n --> 1\nlantejas --> 1\naquellas --> 1\nsalpic\u00f3n --> 1\ndeja --> 1\nama --> 1\ncartas --> 1\npor --> 1\naquellos --> 1\nnarraci\u00f3n --> 1\nLa --> 1\nd\u00edas --> 1\ntanta --> 1\nduelos --> 1\nQuieren --> 1\no --> 1\nedad --> 1\nejercicio --> 1\nvendi\u00f3 --> 1\nentricadas --> 1\nd\u00e9l --> 1\nha --> 1\nera --> 1\nconjeturas --> 1\nsembradura --> 1\ntiempo --> 1\nllamaba --> 1\nmanera --> 1\nquiero --> 1\nolvid\u00f3 --> 1\nmerece --> 1\nprosa --> 1\nquebrantos --> 1\ntodo --> 1\nme --> 1\ncincuenta --> 1\ndonde --> 1\nlanza --> 1\nclaridad --> 1\ncompuso --> 1\ntan --> 1\ndivinidad --> 1\npantuflos --> 1\npoco --> 1\nlugar --> 1\nsalga --> 1\nflaco --> 1\ntal --> 1\nolla --> 1\nfamoso --> 1\nestrellas --> 1\nconsum\u00edan --> 1\ngalgo --> 1\nllev\u00f3 --> 1\ndella --> 1\nafici\u00f3n --> 1\nEl --> 1\nalgo --> 1"}], "metadata": {"collapsed": false}}, {"source": "# Un \u00faltimo ejercicio\n\nObt\u00e9n un un rdd de las palabras comunes a los textos \"El Quijote\" y \"Viajes por Espa\u00f1a\" que cumplan las siguientes condiciones:\n* que tengan m\u00e1s de 4 letras  \n\nEl rdd ha de estar compuesto por diccionarios, y cada diccionario ha de tener\n* La palabra\n* Cuantas veces aparece en \"El Quijote\"\n* Cuantas veces aparece en \"Viajes por Espa\u00f1a\"\n* Cuantas veces aparece en ambos (es decir, la suma de los dos puntos anteriores)", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 193, "cell_type": "code", "source": "def separador_y_limpieza(linea):\n    palabras = linea.split()\n    palabras = [c for c in palabras if c.isalnum()]\n    return palabras\n\n\nqpath = \"adl://barcelodatalake.azuredatalakestore.net/pruebas/Nacho/quijote-2parrafos.txt\"\nqlines = sc.textFile(qpath)\n\nqcounts = qlines.flatMap(lambda x: separador_y_limpieza(x)).map(lambda palabra: (palabra, 1)).reduceByKey(lambda x,y:x+y)\n\nvpath = \"adl://barcelodatalake.azuredatalakestore.net/pruebas/formacion_abril_2018/viajes_por_espa\u00f1a.txt\"\nvlines = sc.textFile(vpath)\n\nvcounts = vlines.flatMap(lambda x: separador_y_limpieza(x)).map(lambda palabra: (palabra, 1)).reduceByKey(lambda x,y:x+y)\n\nqcounts.join(vcounts).map(lambda t: {'Palabra':t[0], 'Q':t[1][0], 'V':t[1][1], 'Suma':t[1][0]+t[1][1]}).collect()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[{'V': 1, 'Q': 2, 'Suma': 3, 'Palabra': 'para'}, {'V': 6, 'Q': 2, 'Suma': 8, 'Palabra': 'os'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'alg\u00fan'}, {'V': 3, 'Q': 2, 'Suma': 5, 'Palabra': 'como'}, {'V': 6, 'Q': 12, 'Suma': 18, 'Palabra': 'los'}, {'V': 21, 'Q': 16, 'Suma': 37, 'Palabra': 'y'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'nombre'}, {'V': 3, 'Q': 7, 'Suma': 10, 'Palabra': 'su'}, {'V': 3, 'Q': 6, 'Suma': 9, 'Palabra': 'con'}, {'V': 5, 'Q': 2, 'Suma': 7, 'Palabra': 'del'}, {'V': 1, 'Q': 3, 'Suma': 4, 'Palabra': 'vuestra'}, {'V': 7, 'Q': 4, 'Suma': 11, 'Palabra': 'el'}, {'V': 2, 'Q': 4, 'Suma': 6, 'Palabra': 'no'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'bien'}, {'V': 16, 'Q': 8, 'Suma': 24, 'Palabra': 'en'}, {'V': 12, 'Q': 21, 'Suma': 33, 'Palabra': 'que'}, {'V': 1, 'Q': 2, 'Suma': 3, 'Palabra': 'muchas'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'este'}, {'V': 3, 'Q': 2, 'Suma': 5, 'Palabra': 'lo'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'haber'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'La'}, {'V': 4, 'Q': 7, 'Suma': 11, 'Palabra': 'a'}, {'V': 3, 'Q': 1, 'Suma': 4, 'Palabra': 'donde'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'aquellas'}, {'V': 2, 'Q': 4, 'Suma': 6, 'Palabra': 'las'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'casi'}, {'V': 2, 'Q': 1, 'Suma': 3, 'Palabra': 'algo'}, {'V': 13, 'Q': 1, 'Suma': 14, 'Palabra': 'por'}, {'V': 1, 'Q': 1, 'Suma': 2, 'Palabra': 'd\u00edas'}, {'V': 14, 'Q': 12, 'Suma': 26, 'Palabra': 'la'}, {'V': 3, 'Q': 1, 'Suma': 4, 'Palabra': 'o'}, {'V': 2, 'Q': 1, 'Suma': 3, 'Palabra': 'todo'}, {'V': 36, 'Q': 38, 'Suma': 74, 'Palabra': 'de'}, {'V': 3, 'Q': 4, 'Suma': 7, 'Palabra': 'un'}, {'V': 4, 'Q': 6, 'Suma': 10, 'Palabra': 'se'}, {'V': 3, 'Q': 2, 'Suma': 5, 'Palabra': 'una'}]"}], "metadata": {"scrolled": true, "collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}